[
  {
    "objectID": "Week_1.html#knowledge-gain-from-the-lecture",
    "href": "Week_1.html#knowledge-gain-from-the-lecture",
    "title": "1  Week_1",
    "section": "1.1 Knowledge gain From the Lecture",
    "text": "1.1 Knowledge gain From the Lecture\n\nThis week’s learning journey into the realm of remote sensing, unveiling the intricate world of electromagnetic waves and their interactions with Earth’s surface. Remote sensing, as I’ve learned, involves acquiring information about objects or areas from a distance, typically from aircraft or satellites, and is a vital tool in understanding our planet.\n\n\n1.1.1 Foundational Concepts of Remote Sensing\n\nOne of the foundational concepts we explored was the difference between passive and active remote sensing. Passive remote sensing relies on natural energy, usually from the sun, whereas active remote sensing systems emit their own energy to illuminate objects. This distinction is crucial for understanding the varied applications of remote sensing technologies like LiDAR, radar, and satellite imagery in observing earth’s landscapes, urban areas, and atmospheric conditions.\n\n\n\n\n\nPassive & Active Remote Sensing, Source: Abeer Nazar Abdul-Hameed\n\n\n\n\n\n\n\n1.1.2 Electromagnetic Waves: Core of Remote Sensing\n\nMuch of our learning knowledge focused on electromagnetic waves, the heart of remote sensing. These waves, a form of energy that propagates through electric and magnetic fields, are the backbone of how remote sensors collect data. Remote sensing is based on this principle to detect the reflection of electromagnetic waves by surface objects and their emission of electromagnetic waves, so as to extract information about these objects and complete the identification of objects at a distance.\n\n\n\n\n\n\nThe Electromagnetic Spectrum, Source: Cssonawala\n\n\n\n\n\n\n1.1.3 Interactions with Earth’s Surface\n\nWe have looked at the complexities of how electromagnetic radiation (EMR) interacts with the Earth’s surface. EMR can be absorbed, transmitted or scattered by the surface and atmosphere. Scattering, in particular, explains phenomena such as the blue colour of the sky or the blackness of the lunar sky due to the absence of an atmosphere.\n\n\n\n\n\n\nInteraction of EMR with Earth’s Surface, Source: Geographicbook\n\n\n\n\n\n\n1.1.4 Synthetic Aperture Radar (SAR) and its Applications\n\nThe Synthetic Aperture Radar (SAR) was another fascinating topic. SAR’s ability to ‘see through clouds’ using longer wavelengths is revolutionary, offering a consistent observation capability irrespective of weather conditions. This technology’s potential in areas like topography, vegetation analysis, and urban planning is immense, showcasing how advanced remote sensing techniques can overcome environmental challenges\nThe study of SAR data also introduced us to the concept of polarization, a property of electromagnetic waves that describes their oscillation direction. This property is crucial for understanding how radar signals interact with different surface properties and is instrumental in enhancing the efficiency and bandwidth of communications.\n\n\n\n1.1.5 Technical Insights and Data Formats in Remote Sensing\n\nOn the technical side, we learned about the different data formats used in remote sensing, such as GeoTIFF, and the importance of resolutions - spatial, spectral, temporal and radiometric. These resolutions define the quality and type of data acquired, and influence how effectively we can interpret and use the data for various applications such as land cover mapping and environmental monitoring.\nIn the literature, these concepts are used to analyse environmental change, urban development and geological features. Understanding remote sensing data and their interpretation is essential for researchers and policy makers to make informed decisions."
  },
  {
    "objectID": "Week_1.html#reflecting-from-the-practical",
    "href": "Week_1.html#reflecting-from-the-practical",
    "title": "1  Week_1",
    "section": "1.2 Reflecting From the Practical",
    "text": "1.2 Reflecting From the Practical\n\n1.2.1 Using of SNAP\nI am still trying to figure out how to fix the bug with snap while using the MacOS."
  },
  {
    "objectID": "Week_1.html#reflecting-from-the-lecture",
    "href": "Week_1.html#reflecting-from-the-lecture",
    "title": "1  Week_1",
    "section": "1.2 Reflecting From the Lecture",
    "text": "1.2 Reflecting From the Lecture\n\nReflecting on the week’s learning, I am struck by the profound impact that remote sensing has on our understanding of the world. The ability to observe and analyse our planet from a distance provides a unique perspective that is not possible through ground-based observations alone. It literally allows us to see the bigger picture, providing a comprehensive understanding of large-scale environmental patterns, urban development and even climate change.\nRemote sensing is interdisciplinary in nature. It’s not just a tool for geographers or environmental scientists; it integrates physics, engineering, environmental science and even policy-making. This integration demonstrates the collaborative effort required to address complex global issues. For example, remote sensing data can inform policies on urban planning, agricultural practices and disaster management, making it a critical tool for sustainable development.\nAnother aspect is the rapid evolution of remote sensing technology. With innovations such as high-resolution imagery and real-time data analysis, the potential applications of this technology are expanding at an incredible rate."
  },
  {
    "objectID": "Week_1.html#reference",
    "href": "Week_1.html#reference",
    "title": "1  Week_1",
    "section": "1.4 Reference",
    "text": "1.4 Reference\n\n\n\n\nAdhikary, Saju, Benukar Biswas, Manish Kumar Naskar, Bishal Mukherjee, Aditya Pratap Singh, Kousik Atta, Saju Adhikary, et al. 2022a. “Remote Sensing for Agricultural Applications.” In. IntechOpen. https://doi.org/10.5772/intechopen.106876.\n\n\n———, et al. 2022b. “Remote Sensing for Agricultural Applications.” In. IntechOpen. https://doi.org/10.5772/intechopen.106876.\n\n\nHu, Jun, Lei Zhang, Changwook Lee, and Rong Gui. 2022. “Editorial: Advanced Big SAR Data Analytics and Applications.” Frontiers in Environmental Science 10. https://www.frontiersin.org/articles/10.3389/fenvs.2022.1063376.\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science 93 (12): 1747–66. https://www.jstor.org/stable/24102069.\n\n\nTorres Gil, Leydy K., David Valdelamar Martínez, and Manuel Saba. 2023. “The Widespread Use of Remote Sensing in Asbestos, Vegetation, Oil and Gas, and Geology Applications.” Atmosphere 14 (1): 172. https://doi.org/10.3390/atmos14010172.\n\n\nZhou, Jianming. 2023. “Application of Remote Sensing Technology in Urban Environment Monitoring.” In. CRC Press.\n\n\nZhou, Qifeng. 2023. “Application of Remote Sensing Technologies in Environmental Monitoring and Geological Surveys.” Applied and Computational Engineering 3 (May): 178–85. https://doi.org/10.54254/2755-2721/3/20230403."
  },
  {
    "objectID": "Week_3.html#knowledge-gain-from-the-lecture",
    "href": "Week_3.html#knowledge-gain-from-the-lecture",
    "title": "3  Week_3",
    "section": "3.1 Knowledge gain From the Lecture",
    "text": "3.1 Knowledge gain From the Lecture\n\nThis week’s study focuses on remote sensing image processing, with the main areas of interest being geometric corrections, atmospheric corrections, orthometric corrections, radiometric corrections and various image enhancement techniques.\n\n\n3.1.1 Correction\n\n3.1.1.1 Geometric Correction\n\nGeometric correction is the basis of image correction and addresses image distortion due to sensor and other factors. The goal of this correction is to make the image conform to the selected map projection system, adjusting the geometry of the entire image by modelling the relationship between control points (points with known geographic coordinates) on the image and the image coordinates.\n\n\n\n\n\n\nHow Geometric Correction Work, Source: Xiaopeng\n\n\n\n\n\n\n3.1.1.2 Atmospheric Correction\n\nAtmospheric corrections are essential to address distortions and inaccuracies in remotely sensed images caused by variations in the Earth’s atmosphere, sensor angles, and terrain. Several different correction methods were learned in class: Relative Atmospheric Correction, Pseudo-Invariant Correction, Absolute Atmospheric Correction, and Empirical Line Correction.\n\nThe different methods and characteristics of atmospheric correction have been summarised in the table below:\n\n\n\n\n\n\n\n\nMethod\nKey Steps\nFeature\n\n\n\n\nRelative\nSpectrally stable landmarks, linear relations, band operation\nConsistency between images\n\n\nAbsolute\nComplex models for atmospheric effects, surface reflectance\nPrecise, accurate surface information\n\n\nPseudo-Invariant\nHigh-quality reference, PIF, linear regression\nStable reference points, reduce atmospheric effects\n\n\nEmpirical Line\nGround reflectance, average DN values, linear regression\nUtilizes ground data for satellite correction\n\n\n\n\n\n3.1.1.3 Radiometric Correction\n\nThe role of radiometric correction in adjusting the pixel values of satellite imagery to accurately reflect the radiation at the Earth’s surface has been investigated. The main objective of radiometric correction is to convert the raw digital numbers (DN) acquired into physically meaningful units such as radiance or reflectance. This conversion is critical because DN values are arbitrary and can vary between sensors, acquisitions and platforms, making it difficult to consistently compare and analyse data. By converting DN values to radiance or reflectance, radiometric correction enables quantitative measurements and meaningful comparisons between different images and sensors.\n\n\n\n\n\n\nProcess of Radiometric Calibration\n\n\n\n\n\n\n3.1.1.4 Reflection of Collerallation\n\nIn the course of study, certain questions arise as to when atmospheric corrections are needed and whether or not the images we buy or otherwise acquire have been atmospherically corrected. In layman’s terms, if we need to use an image that truly reflects the Sun’s radiation for quantitative inversion or to obtain information about the Earth, accurately identify features, etc., then we need to make an atmospheric correction. When we buy an image, the description document says that it is radiometrically corrected. In fact, this radiometric correction refers to the coarse radiometric correction, which is just a systematic atmospheric correction, which has the same meaning as the systematic geometric correction.\nAt the moment there are many models and methods on the market, since there are how many methods, then there is the problem of method selection. Here is a summary for reference:\n\nif it is a fine quantitative study, then choose the atmospheric correction method based on the radiative transfer model.\nif you are doing dynamic monitoring, then you can choose relative atmospheric correction or simpler methods.\nif the parameters are missing, there is no choice but to choose the simpler method.\n\n\n\n\n\n3.1.2 Data Join Sets/Enhancement\n\n3.1.2.1 Data Join Sets\n\nAn area may need more than one satellite image to be spliced, using mosaic in the R package for the two datasets to be spliced. When selecting images, try to select two images with similar time and date, due to the different date of the image, the image display is different, the reason may be (cloud cover, sunshine), need to use the image increase technique to operate.\n\n\n\n\n3.1.3 Enhancement\n\n3.1.3.1 Ratio\n\nThe ratio is the difference between two spectral bands with a specific spectral response, using CampTown’s data for NDVI (The Normalised Difference Vegetation Index) The Normalised Difference Vegetation Index is based on the fact that healthy and green vegetation reflects more in the near infrared but absorbs in the red wavelengths. Therefore, the red wavelength band is used for the operation in the formula below.\n\\[\nNDVI = \\frac{NIR - Red}{NIR + Red}\n\\]\nThe following figure shows the image after manipulation for NDVI (a. After NDVI Formula, b. Extraction only if NDVI is equal to or greater than 0.2)\n\n\n\n\n\n\nProcess of Radiometric Calibration\n\n\n\n\nThis case above can be applied to other index calculations such as NDWI (Normalised Difference Water Index) and NDDI (Normalised Difference Drought Index). These are calculated using the reflectance of different objects in different light waves.\n\n\n3.1.3.2 Texture\n\nThe extraction method of texture features is relatively simple, it is to use an active window to slide continuously on the image, calculate the variance, mean, maximum, minimum and the difference between the two and the information entropy in the window, etc., respectively, to form the corresponding texture image, when the spectral characteristics of the target are relatively close to each other, the texture features can play a positive role in distinguishing the target. When the spectral characteristics of the target are close, the texture features can play a positive role in distinguishing the target. After selecting the appropriate dynamic range of the data and extracting the texture features, the texture features of the image can be highlighted, which is conducive to the extraction of constructive information. Below is an example of a texture treatment for the Cape Town area.\n\n\n\n\n\n\nAfter of Texture Process"
  },
  {
    "objectID": "Week_3.html#application-in-literature",
    "href": "Week_3.html#application-in-literature",
    "title": "3  Week_3",
    "section": "3.2 Application in Literature",
    "text": "3.2 Application in Literature\n\nImage correction is a critical step in remote sensing to ensure the accuracy and usability of data obtained from satellite or aerial imagery. Radiometric correction involves adjusting digital image data to correct for sensor noise, sensor response variations and atmospheric conditions. Various methods have been proposed for radiometric correction of remote sensing images. Duan(2014) proposed a radiation correction method that replaces artificial targets with standard ground objects, resulting in accurate and precise correction. Tarasenkov(2019) developed a program complex for atmospheric correction of satellite images that considers radiation polarization.\nGeometric correction corrects the image so that the proportions are uniform throughout the image. This correction is essential for accurate mapping and measurement tasks. Geometric correction is useful for removing spatial distortion, aligning images of the same sample, and stitching overlapping images together.(Yan et al. 2023). Various methods and devices have been developed for geometric correction, including those based on orthographic images and homonymous point matching. These techniques improve the accuracy and stability of the geometric information in the corrected image(Özciḣan et al. 2023).\nAtmospheric correction is necessary for various remote sensing applications to improve the accuracy and reliability of derived information. Different algorithms, such as 6S, FLAASH, DOS, LaSRC, and Sen2Cor, have been evaluated for their performance in atmospheric correction(Muchsin et al. 2023). Atmospheric corrections are essential for a variety of applications such as aircraft navigation, astronomical observations and accurate estimation of cloud heights(Shah, Raval, and Divakaran 2022). It ensures that satellite images provide reliable and accurate results by taking into account atmospheric disturbance(Jonah and Aketi, n.d.).\nImage enhancement technology aims to improve the quality and visual effect of remote sensing images. Various methods have been proposed to enhance these images, including histogram modification, transform domain methods, and hybrid approaches. Histogram modification methods focus on modifying the histogram of the input image to achieve a more uniform distribution, resulting in better contrast improvement. Transform domain methods apply specific transforms to enhance the image in the transform domain, leading to better edge enhancement and color preservation(Wang et al. 2023). Hybrid methods, which combine histogram modification and transform domain methods, have shown higher potential in enhancing remote sensing images(Deng et al. 2023). These enhancement techniques are crucial for image interpretation, improving image segmentation accuracy, and facilitating downstream tasks."
  },
  {
    "objectID": "Week_3.html#personal-reflection",
    "href": "Week_3.html#personal-reflection",
    "title": "3  Week_3",
    "section": "3.3 Personal Reflection",
    "text": "3.3 Personal Reflection\n\nAs a student diving deep into remote sensing image correction technologies, it truly broadened my knowledge and also deepened my understanding of its complexities and challenges. I realized that although these technologies are crucial for enhancing the quality of image data, implementing them is not as straightforward as it might seem. For instance, radiometric correction sounds advanced, but if we lack precise ground objects for reference, the accuracy of the whole process becomes questionable. This got me thinking about whether we need to develop more adaptive correction methods that can adjust to varying conditions.\nThen there’s geometric correction, which indeed seems capable of solving issues with inconsistent image proportions, but if our initial data is not of good quality, or the processing is too complex, how do we balance precision with practical feasibility? It made me realize that sometimes we might need to find a balance between ideal accuracy and operational simplicity.\nWhen it comes to atmospheric correction, I was intrigued by the performance differences of various algorithms under specific conditions. This highlights the importance of selecting the right correction algorithm for reliable data, and also hints at the necessity for more detailed comparisons of algorithms in future research.\nRegarding image enhancement technologies, while they can make images appear clearer, I also began to worry about whether these techniques might inadvertently alter some crucial features in the images. This made me recognize the need to carefully maintain the authenticity of images while pursuing visual improvements.\nOverall, delving into remote sensing image correction technologies not only taught me about the importance of these techniques but also exposed the various challenges that need to be overcome in practical applications. It made me more aware that alongside technological innovation, a careful evaluation and improvement of existing methods are necessary to ensure we can enhance data quality while maintaining operational feasibility and data authenticity."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "3  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliography",
    "section": "",
    "text": "‘Remote Sensing - NASA’, n.d. https://www.nasa.gov/directorates/somd/space-communications-navigation-program/remote-sensing/.\n‘Types Of Remote Sensing: Devices And Their Applications’, 18 November 2020. https://eos.com/blog/types-of-remote-sensing/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Remote sensing learning Diary",
    "section": "",
    "text": "Preface\nThis is a Learning Diary of CASA0023 Remotely Sensing Cities and Environments.\n\n\nAdd more in the future"
  },
  {
    "objectID": "Week_3.html#application-in-literature-unfinished",
    "href": "Week_3.html#application-in-literature-unfinished",
    "title": "3  Week_3",
    "section": "3.2 Application in Literature (Unfinished)",
    "text": "3.2 Application in Literature (Unfinished)\nImage correction is a critical step in remote sensing to ensure the accuracy and usability of data obtained from satellite or aerial imagery. Radiometric correction involves adjusting digital image data to correct for sensor noise, sensor response variations and atmospheric conditions. Various methods have been proposed for radiometric correction of remote sensing images. Duan(2014) proposed a radiation correction method that replaces artificial targets with standard ground objects, resulting in accurate and precise correction@duan2014.Tarasenkov et al. developed a program complex for atmospheric correction of satellite images that considers radiation polarization@tarasenkov2019.\n几何校正对图像进行校正，使整个图像的比例均匀。这种校正对于准确的绘图和测量任务至关重要。几何校正对于消除空间失真、对齐同一样本的图像以及将重叠图像缝合在一起非常有用@yan2023。人们已经开发了各种用于几何校正的方法和设备，包括基于正射图像和同名点匹配的方法和设备。这些技术提高了校正图像中几何信息的准确性和稳定性。; Özciḣan et al. (2023)\nAtmospheric correction is necessary for various remote sensing applications to improve the accuracy and reliability of derived information. Different algorithms, such as 6S, FLAASH, DOS, LaSRC, and Sen2Cor, have been evaluated for their performance in atmospheric correction@muchsin2023. 大气校正对于飞机导航、天文观测和准确估计云高等各种应用至关重要@shah2022。它通过考虑大气干扰来确保卫星图像提供可靠且精确的结果。; “Atmospheric Correction of Landsat Image | Journal of Environmental and Geographical Studies” (n.d.)\nimage enhancement technology aims to improve the quality and visual effect of remote sensing images. Various methods have been proposed to enhance these images, including histogram modification, transform domain methods, and hybrid approaches. Histogram modification methods focus on modifying the histogram of the input image to achieve a more uniform distribution, resulting in better contrast improvement. Transform domain methods apply specific transforms to enhance the image in the transform domain, leading to better edge enhancement and color preservation@wang2023. Hybrid methods, which combine histogram modification and transform domain methods, have shown higher potential in enhancing remote sensing images@deng2023. These enhancement techniques are crucial for image interpretation, improving image segmentation accuracy, and facilitating downstream tasks."
  },
  {
    "objectID": "Week_3.html#personal-reflection-unfinished",
    "href": "Week_3.html#personal-reflection-unfinished",
    "title": "3  Week_3",
    "section": "3.3 Personal Reflection (Unfinished)",
    "text": "3.3 Personal Reflection (Unfinished)\n\nAs a student diving deep into remote sensing image correction technologies, it truly broadened my knowledge and also deepened my understanding of its complexities and challenges. I realized that although these technologies are crucial for enhancing the quality of image data, implementing them is not as straightforward as it might seem. For instance, radiometric correction sounds advanced, but if we lack precise ground objects for reference, the accuracy of the whole process becomes questionable. This got me thinking about whether we need to develop more adaptive correction methods that can adjust to varying conditions.\nThen there’s geometric correction, which indeed seems capable of solving issues with inconsistent image proportions, but if our initial data is not of good quality, or the processing is too complex, how do we balance precision with practical feasibility? It made me realize that sometimes we might need to find a balance between ideal accuracy and operational simplicity.\nWhen it comes to atmospheric correction, I was intrigued by the performance differences of various algorithms under specific conditions. This highlights the importance of selecting the right correction algorithm for reliable data, and also hints at the necessity for more detailed comparisons of algorithms in future research.\nRegarding image enhancement technologies, while they can make images appear clearer, I also began to worry about whether these techniques might inadvertently alter some crucial features in the images. This made me recognize the need to carefully maintain the authenticity of images while pursuing visual improvements.\nOverall, delving into remote sensing image correction technologies not only taught me about the importance of these techniques but also exposed the various challenges that need to be overcome in practical applications. It made me more aware that alongside technological innovation, a careful evaluation and improvement of existing methods are necessary to ensure we can enhance data quality while maintaining operational feasibility and data authenticity."
  },
  {
    "objectID": "Week_4.html#summary-of-the-policy-and-city",
    "href": "Week_4.html#summary-of-the-policy-and-city",
    "title": "4  Week_4",
    "section": "4.1 Summary of the Policy and City",
    "text": "4.1 Summary of the Policy and City\n\n4.1.1 Dublin City\n\nDublin’s exposure to flood risk is influenced by a number of factors, including urbanisation and climate change(Paranunzio et al. 2022a). This city on the coast has a complex system of rivers, canals, surface water sewers, sewers and urban watercourses, making it particularly sensitive to flooding. Causes of flooding include sea level rise, runoff water, heavy rainfall, extreme events, storms and tidal fluctuations. Flooding events caused by extreme weather have increased significantly over the last decade and this is expected to continue. It is also expected that the number of days of heavy rainfall per year will increase, leading to an increased risk of fluvial (fluvial) and pluvial (pluvial) flooding(Paranunzio et al. 2022b).\nAlso sea level rise in Dublin is an important consideration. As a result of climate change, Dublin City Council has undertaken a review of existing coastal flood defences to ensure they provide protection for the city region. Records show that average sea levels in Dublin Bay have risen faster than the global average between 2000 and 2016(Shoari Nejad et al. 2022).\n\n\n\n4.1.2 Policy Background\n\nThe Dublin City Development Plan (DCDP) 2016-2022 includes a key component, the Strategic Flood Risk Assessment (SFRA). This plan is intended to guide the direction and location of development in Dublin City over the life of the plan(Dublin City Council 2020a). It provides an integrated and coherent spatial framework to ensure that the city develops in an inclusive manner, whilst enhancing the quality of life for its citizens and making Dublin a more attractive place to live and work. This plan was adopted by Dublin City Council at a special meeting on 23rd September 2016 and came into force on 21st October 2016\nThe Strategic Flood Risk Assessment (SFRA) is Volume 7 of the Dublin City Development Plan (DCDP) and is specifically designed to assess and manage flood risk. The purpose of the assessment is to comply with the requirements of the Floods Directive and flood risk/hazard maps are being produced to enable the development of a comprehensive Flood Risk Management Scheme (FRAMS)(Dublin City Council 2020b). The plan also addresses project-specific flood defense infrastructure to protect the more vulnerable parts of the city.\n\n\n\n\n\nExample of Flood Risk Assessment Map( Liffey: Sean Heuston Br. – Sarah Bridge, South Circular Road), Source: Dublin City Council\n\n\n\n\n\n\n\n\n\nExample of Flood Alleviation Programme ( Liffey: Sean Heuston Br. – Sarah Bridge, South Circular Road), Source: Dublin City Council\n\n\n\n\nDublin City Council is also working on various projects to address areas of the city that are susceptible to coastal flooding during extreme events and therefore require new protection works. These projects include the South Bank Flood Defence Project, Sandymount Promenade and Flood Defence Project(O’Connell, n.d.), and Clontarf Promenade Development and Flood Defence Project(Cooke et al. 2005). As part of the Sutton to Sandycove promenade and cycle path project(Lyne 2021), the part of the scheme nearer to Bull Island has commenced and includes flood defence work.\n\n\n\n4.1.3 Policy & Objectives\n\n\nClimate Change Mitigation and Adaptation: Implementing strategies to address climate change impacts on flooding.\nStrategic Flood Risk Assessment: Conducting assessments to inform and improve the city’s flood defenses.\nSustainable Environmental Infrastructure: Mitigating flood and drought effects through environmental assessments and planning.\nDevelopment Compliance: Ensuring new developments respect and enhance existing flood defense mechanisms.\nSustainable Urban Drainage Systems (SUDS): Mandating SUDS in new developments for better water management.\nSite-Specific Flood Risk Assessments: Requiring detailed flood risk analyses for all new development proposals.\nCollaborative Flood Management: Working with neighboring authorities and incorporating catchment-based flood risk management plans.\nGreen Infrastructure Integration: Utilizing green spaces for flood management, biodiversity, and recreation, in line with SUDS principles."
  },
  {
    "objectID": "Week_4.html#how-should-the-remotely-sense-data-address-the-policy-objectives",
    "href": "Week_4.html#how-should-the-remotely-sense-data-address-the-policy-objectives",
    "title": "4  Week_4",
    "section": "4.2 How should the remotely sense data address the policy objectives",
    "text": "4.2 How should the remotely sense data address the policy objectives\n\n洪水风险映射：使用遥感技术在洪水预防方面可以提对其的监测，提供实时数据，并且可以对洪水监测的时效性提供保障(Diao, Sang, and Wang 2022)。可以利用雷达数据重点提取往期洪水淹没区域，分析洪水时空变化特征。同时可以来绘制河流洪水影响的高分辨率地图，并且使用Google Earth Engine 来处理地理空间数据，从而进行洪水风险管理以及洪涝灾害进行监测分析(Colacicco et al. 2022)。\n影响评估和恢复规划：使用遥感数据可以促进洪水后影响评估和恢复规划。遥感技术为洪水淹没绘图提供了清晰的空间信息，这对于及时评估损失和规划恢复工作至关重要。通过利用事件后近实时（NRT）遥感数据与实时（RT）志愿者地理信息（VGI）相结合，可以生成洪水概率图来识别需要紧急关注的区域(Luo, Liao, and Shen 2023)。此外，无人机捕获的航空图像可用于重建 3 维模型和数字高程模型，以进行洪水建模和损害评估(Whitehurst et al. 2022)。基于洪水前和洪水后数字高程模型的模拟可以帮助预测未来降雨事件的影响并指导恢复工作 (Sajjad et al. 2023)。\n生态系统和流域管理：使用遥感数据可以改善洪水后影响流域管理。卫星图像等遥感技术可用于监测和评估洪水造成的损害，包括洪水的范围、基础设施的破坏以及土地覆盖和土地利用的变化(Sridharan, Kumar, and Madhur Kumar 2022)。这些信息有助于评估流域开发干预措施的有效性并确定需要改进的领域。地理信息系统（GIS）可以与遥感数据集成，以增强利益相关者和公众对流域规划过程的参与(Quinn et al. 2022)。通过利用 GIS 和遥感数据集，利益相关者可以获得有价值的信息以进行决策和规划(Quinn et al. 2022)。"
  },
  {
    "objectID": "Week_4.html#flooding-data-come-from",
    "href": "Week_4.html#flooding-data-come-from",
    "title": "4  Week_4",
    "section": "4.3 Flooding Data come from",
    "text": "4.3 Flooding Data come from\n爱尔兰公共工程办公室（Office of Public Works, OPW）提供了全国指示性河流地图（National Indicative Fluvial Mapping），这是洪水风险管理的关键工具。这些地图提供了关于洪水范围、危险和风险的详细信息，包括频繁、轻微的洪水事件到非常罕见、极端事件的评估和映射。\n\n\n\n\nColacicco, Rosa, Alberto Refice, Raffaele Nutricato, Annarita D’Addabbo, Davide Oscar Nitti, and Domenico Capolongo. 2022. “High Spatial and Temporal Resolution Flood Monitoring Through Integration of Multisensor Remotely Sensed Data and Google Earth Engine Processing.” https://doi.org/10.5194/egusphere-egu22-4403.\n\n\nDiao, Chao, Guoqing Sang, and JunNuo Wang. 2022. “Research on the Application of Remote Sensing Monitoring to Flood Monitoring Based on Sentinel-1A in Linyi City.” 2022 3rd International Conference on Geology, Mapping and Remote Sensing (ICGMRS), April. https://doi.org/10.1109/icgmrs55602.2022.9849305.\n\n\nLuo, Huanzhang, Jingjuan Liao, and Guozhuang Shen. 2023. “Combining Remote Sensing and Social Media Data for Flood Mapping: A Case Study in Linhai, Zhejiang Province, China.” Journal of Applied Remote Sensing 17 (2): 024507. https://doi.org/10.1117/1.JRS.17.024507.\n\n\nQuinn, Nigel W. T., Vamsi Sridharan, John Ramirez-Avila, Sanaz Imen, Huilin Gao, Rocky Talchabhadel, Saurav Kumar, and Walter McDonald. 2022. “Applications of GIS and Remote Sensing in Public Participation and Stakeholder Engagement for Watershed Management.” Socio-Environmental Systems Modelling 4 (October): 18149–49. https://doi.org/10.18174/sesmo.18149.\n\n\nSajjad, Asif, Jianzhong Lu, Xiaoling Chen, Chikondi Chisenga, and Nausheen Mazhar. 2023. “Rapid Assessment of Riverine Flood Inundation in Chenab Floodplain Using Remote Sensing Techniques.” Geoenvironmental Disasters 10 (1): 9. https://doi.org/10.1186/s40677-023-00236-7.\n\n\nSridharan, Vamsi Krishna, Saurav Kumar, and Swetha Madhur Kumar. 2022. “Can Remote Sensing Fill the United States’ Monitoring Gap for Watershed Management?” Water 14 (13): 1985. https://doi.org/10.3390/w14131985.\n\n\nWhitehurst, Daniel, Kunal Joshi, Kevin Kochersberger, and James Weeks. 2022. “Post-Flood Analysis for Damage and Restoration Assessment Using Drone Imagery.” Remote Sensing 14 (19): 4952. https://doi.org/10.3390/rs14194952."
  },
  {
    "objectID": "Week_4.html#applications",
    "href": "Week_4.html#applications",
    "title": "4  Week_4",
    "section": "4.2 Applications",
    "text": "4.2 Applications\n\n4.2.1 How Remote Sensing Data Should Address Policy Objectives\n\n\nFlood Risk Mapping: The use of remote sensing technology in flood prevention can mention its monitoring, provide real-time data, and can provide a guarantee of the timeliness of flood monitoring(Diao, Sang, and Wang 2022).Radar data can be used to focus on the extraction of past flood inundation areas, and to analyse the characteristics of spatial and temporal changes in flooding. At the same time, it can be used to draw high-resolution maps of the impact of river flooding, and use Google Earth Engine to process geospatial data, so as to carry out flood risk management and monitoring of flood disasters(Colacicco et al. 2022).\nImpact Assessment and Recovery Planning: The use of remote sensing data can facilitate post-flood impact assessment and recovery planning. Remote sensing provides clear spatial information for flood inundation mapping, which is critical for timely damage assessment and planning of recovery efforts. By using near real-time (NRT) remote sensing data after a flood event in combination with real-time (RT) volunteer geographic information (VGI), probabilistic flood maps can be generated to identify areas requiring urgent attention(Luo, Liao, and Shen 2023)。In addition, aerial imagery captured by drones can be used to reconstruct 3D models and digital elevation models for flood modelling and damage assessment(Whitehurst et al. 2022). Simulations based on pre- and post-flood digital elevation models can help predict the impact of future rainfall events and guide recovery efforts (Sajjad et al. 2023).\nWatershed management:The use of remote sensing data can improve post-flood impact watershed management. Remote sensing technologies such as satellite imagery can be used to monitor and assess damage caused by floods, including the extent of flooding, infrastructure damage, and changes in land cover and land use (Sridharan, Kumar, and Madhur Kumar 2022). This information helps to assess the effectiveness of watershed development interventions and identify areas for improvement. Geographic Information Systems (GIS) can be integrated with remote sensing data to enhance stakeholder and public participation in the watershed planning process(Quinn et al. 2022). By utilising GIS and remotely sensed datasets, stakeholders can gain valuable information for decision-making and planning(Quinn et al. 2022).\n\n\n\n\n4.2.2 Connecting to the Big Picture\n\n4.2.2.1 With Local Development\n\nThis policy and its associated Strategic Flood Risk Assessment (SFRA) has had a significant impact on the Local Development Strategy, focussing on sustainable infrastructure and flood risk management. The Plan emphasises the importance of managing surface water drainage and the potential impact of local development on downstream watercourses such as the River Carmichael and the River Liffey. It emphasises the need for additional infrastructure to support development sites, Sustainable Urban Drainage Systems (SuDS) to effectively manage surface water run-off and improve water quality(Dublin City Council 2020c).\nSuDS infrastructure has been highlighted as a core strategy in the Local Plan, designed to manage surface water sustainably, whilst ensuring that there is no increased risk of flooding either upstream or downstream. The approach includes a variety of SuDS features such as detention ponds, infiltration trenches and depressions designed to reduce runoff and improve water quality. This holistic water management strategy supports the vision of creating a vibrant and sustainable urban area in Dublin(South Dublin City Council, n.d.).\nIn addition, the Plan sets out the need for a comprehensive review of existing and future infrastructure (including sewerage and water supply networks) to accommodate development while managing environmental impacts. This includes considerations for upgrading existing infrastructure (e.g. water mains and sewerage systems) to meet the needs of new development(South Dublin City Council, n.d.).\n\n\n\n\n\nDublin Flooding Protection Location, Author: I.Cooke\n\n\n\n\n\n\n\n4.2.2.2 With Global Agenda\n\nIn order to address the identified flood risks, the Plan outlines several strategies and measures. These include the Dublin Coastal Flood Protection Project and participation in the EU Interreg Programme IIIB SAFER project which focuses on coastal flood risk. Dublin City Council works closely with the Office of Public Works (OPW), Ireland’s lead agency for flood risk management, under the Catchment Flood Risk Assessment and Management (CFRAM) programme. The programme is at the heart of Ireland’s medium to long term strategy for flood risk reduction and management and involves the production of detailed flood risk maps and management plans for Dublin’s main rivers and coastal areas.(Dublin City Council 2020c)。\nMore broadly the Plan can explicitly link its objectives and strategies to the relevant SDGs, particularly those relating to Sustainable Cities and Communities (SDG 11), Climate Action (SDG 13) and Water Resources Management (SDG 6). By integrating the flood risk management strategy with these objectives, Dublin can demonstrate its commitment to the global sustainable development agenda while addressing local challenges(Anthony F and Max, n.d.)。\nSchemes should ensure compliance with EU Directives relevant to flood risk management, such as the EU Floods Directive, which requires Member States to assess and manage flood risk in order to reduce the impact of flooding on human health, the environment, cultural heritage and economic activities. By aligning the SFRA with these directives, Dublin City Council can integrate European standards into local practice, facilitate cross-border co-operation and share best practice(herve 2021)。\n\n\n\n\n4.2.3 Types of Remotely Sense Data Can be use for the Urban Flooding Analysis\n\n4.2.3.1 General Data\n\nThe following table presents general data\n\n\n\n\n\n\n\n\n\nSatellites\nSensors\nSpectral Measurements\nParameter\n\n\n\n\nLandsat 5, 7, 8\nETM+, OLI\nVisible, Near IR, Middle IR,\nReflectance/True Color Image,\n\n\n\n\nThermal IR\nLand Cover, Surface Inundation\n\n\nTRMM & GPM\nMicrowave\nTMI: 10-85 Ghz; GMI: 10-183 GHZ;\nPrecipitation\n\n\n\nRadiometer and\nPR and DPR (Ku and Ka)\n\n\n\n\nRADAR (TMI, PR,\n\n\n\n\n\nGMI, DPR)\n\n\n\n\nTerra & Aqua\nMODIS\nVisible, Near IR, Middle IR\nReflectance/True Color Image,\n\n\n\n\n\nSurface Inundation, Land Cover\n\n\nSNPP\nVIIRS\nVisible, Near IR, Middle IR\nDay/Night Imagery\n\n\nSMAP\nMicrowave\n1.41 GHz\nSoil Moisture\n\n\n\nRadiometer\n\n\n\n\nSentinel 1A and 1B\nSynthetic\nC-Band\nBackscatter/Surface Inundation\n\n\n\nAperture RADAR\n\n\n\n\n\n(SAR)\n\n\n\n\nSpace Shuttle\nSRTM\nC-Band\nTerrain\n\n\n\n\n4.2.3.2 Global Precipitation Measurement (GPM) Mission\nThe Global Precipitation Measurement (GPM) mission plays a crucial role in analyzing global rainfall patterns. It stands at the forefront of flooding trend analysis, thanks to its comprehensive and derived algorithms. Recognized for its unparalleled popularity and reliability, GPM serves as the primary source of data and analysis method for studying precipitation(Skofronick-Jackson et al. 2018).\nGPM represents an international collaboration, involving a network of satellites dedicated to providing advanced observations of rain and snow across the globe. It builds on the achievements of the Tropical Rainfall Measuring Mission (TRMM) and introduces the concept of a Core Observatory. This central satellite is equipped with sophisticated radar and radiometer systems designed to accurately measure precipitation from space. This Core Observatory also plays a pivotal role in standardizing precipitation measurements collected from a constellation of both research and operational satellites, ensuring consistent and reliable data worldwide(Hou 2012).\n\n\n\n\n\n\n\nGlobal Precipitation Measurement (GPM) Mission, Source: NASA\n\n\n\n\n\n\n4.2.3.3 Relative Data Source Link\n\nGPM IMERG Data Access: https://gpm.nasa.gov/data\nPrecipitation Data Access and Analysis: https://giovanni.gsfc.nasa.gov/giovanni/\nMODerate Resolution Imaging Spectroradiometer (MODIS): https://lpdaac.usgs.gov/ or https://search.earthdata.nasa.gov/\nVisible Infrared Imaging Radiometer Suite (VIIRS): https://worldview.earthdata.nasa.gov/\nSoil Moisture Active Passive (SMAP): https://nsidc.org/data/search#keywords=soil+moisture/\nETC……"
  },
  {
    "objectID": "Week_4.html#reflect-unfinished",
    "href": "Week_4.html#reflect-unfinished",
    "title": "4  Week_4",
    "section": "4.3 Reflect (Unfinished)",
    "text": "4.3 Reflect (Unfinished)\n\nIncorporating the application of remote sensing technologies into the context of the Dublin City Development Plan 2016-2022 and its Strategic Flood Risk Assessment (SFRA) not only broadens the scope of my reflection but also enhances the depth of analysis on how technological advancements can be synergized with urban planning and environmental management practices. This expanded consideration allows for a multifaceted approach towards sustainable urban development, emphasizing the critical role of innovative technologies in addressing complex urban challenges.\nRemote sensing, with its capacity to collect detailed environmental data from a distance, presents an invaluable tool for urban planners and policymakers. By facilitating a comprehensive analysis of land use changes, vegetation cover, water bodies, and urban infrastructure, remote sensing data can significantly contribute to informed decision-making processes. For Dublin, leveraging such technologies means the ability to dynamically monitor urban expansion and its impacts on flood risks, assess vulnerabilities across the urban landscape, and develop targeted strategies for flood mitigation and urban resilience.\nThis integration goes beyond traditional planning methods by enabling a proactive rather than reactive approach to urban development and environmental stewardship. The precision and timeliness of data provided by remote sensing can lead to the early identification of potential flood-prone areas, changes in land cover that may affect hydrological cycles, and the effectiveness of existing flood defense structures. Consequently, this facilitates the optimization of land use planning, infrastructure development, and environmental conservation efforts to mitigate flood risks effectively.\nFurthermore, the application of remote sensing technologies underscores the importance of interdisciplinary collaboration and capacity building among urban planners, environmental scientists, and the broader community. Engaging with a diverse range of stakeholders in the interpretation and application of remote sensing data can foster a more inclusive and participatory approach to urban planning. This collaborative framework not only enhances the understanding and management of flood risks but also promotes a shared sense of responsibility and collective action towards sustainable urban development.\nMoreover, the ongoing advancement in remote sensing technologies, including higher resolution imagery, real-time data acquisition, and improved analytical tools, offers new opportunities for innovation in urban planning and environmental management. As cities like Dublin strive to align their development strategies with global sustainability goals, the integration of such technologies becomes increasingly crucial. It allows for a more nuanced understanding of urban ecosystems, the interconnections between human activities and natural processes, and the pathways towards achieving a harmonious balance between urban development and environmental preservation.\nIn summary, the reflection on integrating remote sensing technologies into Dublin’s urban planning and flood risk management efforts highlights the transformative potential of such tools in advancing sustainable urban development. It points towards a future where technology and data-driven insights become central to crafting resilient, inclusive, and sustainable urban landscapes. This expanded perspective not only enriches my understanding of the complexities involved in urban planning but also inspires a forward-looking approach to leveraging technology for the betterment of our urban environments."
  },
  {
    "objectID": "Week_4.html#reference",
    "href": "Week_4.html#reference",
    "title": "4  Week_4",
    "section": "4.4 Reference",
    "text": "4.4 Reference\n\n\n\n\nAnthony F, Pipa, and Bouchet Max. n.d. “Local Leadership Driving Progress on the Sustainable Development Goals.” https://www.brookings.edu/articles/local-leadership-driving-progress-on-the-sustainable-development-goals/.\n\n\nColacicco, Rosa, Alberto Refice, Raffaele Nutricato, Annarita D’Addabbo, Davide Oscar Nitti, and Domenico Capolongo. 2022. “High Spatial and Temporal Resolution Flood Monitoring Through Integration of Multisensor Remotely Sensed Data and Google Earth Engine Processing.” https://doi.org/10.5194/egusphere-egu22-4403.\n\n\nCooke, I, A D Maguire, O McManus, and B Bliek. 2005. “The Dublin Coastal Protection Project.” WIT Transactions on The Built Environment 78.\n\n\nDiao, Chao, Guoqing Sang, and JunNuo Wang. 2022. “Research on the Application of Remote Sensing Monitoring to Flood Monitoring Based on Sentinel-1A in Linyi City.” 2022 3rd International Conference on Geology, Mapping and Remote Sensing (ICGMRS), April. https://doi.org/10.1109/icgmrs55602.2022.9849305.\n\n\nDublin City Council. 2020c. “3.5.3 Climate Change and Flood Risk | Dublin City Council.” https://www.dublincity.ie/dublin-city-development-plan-2016-2022/3-addressing-climate-change/35-policies-and-objectives/353-climate-change-and-flood-risk.\n\n\n———. 2020a. “3.5.3 Climate Change and Flood Risk | Dublin City Council.” https://www.dublincity.ie/dublin-city-development-plan-2016-2022/3-addressing-climate-change/35-policies-and-objectives/353-climate-change-and-flood-risk.\n\n\n———. 2020b. “3.5.3 Climate Change and Flood Risk | Dublin City Council.” https://www.dublincity.ie/dublin-city-development-plan-2016-2022/3-addressing-climate-change/35-policies-and-objectives/353-climate-change-and-flood-risk.\n\n\nherve. 2021. “Global Policies in Local Context: Local Transformation Through International Engagement - Platforma.” https://platforma-dev.eu/global-policies-in-local-context-local-transformation-through-international-engagement/, https://platforma-dev.eu/global-policies-in-local-context-local-transformation-through-international-engagement/.\n\n\nHou, Arthur Y. 2012. “Global Precipitation Measurement (GPM) Mission: Overview and Status,” October. https://ntrs.nasa.gov/citations/20120015575.\n\n\nLuo, Huanzhang, Jingjuan Liao, and Guozhuang Shen. 2023. “Combining Remote Sensing and Social Media Data for Flood Mapping: A Case Study in Linhai, Zhejiang Province, China.” Journal of Applied Remote Sensing 17 (2): 024507. https://doi.org/10.1117/1.JRS.17.024507.\n\n\nLyne, Laura. 2021. “Dublin Residents Fearing Worst as Dream Cycleway at Risk Due to Plan Changes.” https://www.dublinlive.ie/news/dublin-news/residents-fear-dream-sutton-sandycove-20368777.\n\n\nO’Connell, Gerard. n.d. “Sandymount Coastal Flood Defence SchemePhase 1&2.”\n\n\nParanunzio, Roberta, Marco Guerrini, Edward Dwyer, Paul J. Alexander, and Barry O’Dwyer. 2022a. “Assessing Coastal Flood Risk in a Changing Climate for Dublin, Ireland.” Journal of Marine Science and Engineering 10 (11): 1715. https://doi.org/10.3390/jmse10111715.\n\n\n———. 2022b. “Assessing Coastal Flood Risk in a Changing Climate for Dublin, Ireland.” Journal of Marine Science and Engineering 10 (11): 1715. https://doi.org/10.3390/jmse10111715.\n\n\nQuinn, Nigel W. T., Vamsi Sridharan, John Ramirez-Avila, Sanaz Imen, Huilin Gao, Rocky Talchabhadel, Saurav Kumar, and Walter McDonald. 2022. “Applications of GIS and Remote Sensing in Public Participation and Stakeholder Engagement for Watershed Management.” Socio-Environmental Systems Modelling 4 (October): 18149–49. https://doi.org/10.18174/sesmo.18149.\n\n\nSajjad, Asif, Jianzhong Lu, Xiaoling Chen, Chikondi Chisenga, and Nausheen Mazhar. 2023. “Rapid Assessment of Riverine Flood Inundation in Chenab Floodplain Using Remote Sensing Techniques.” Geoenvironmental Disasters 10 (1): 9. https://doi.org/10.1186/s40677-023-00236-7.\n\n\nShoari Nejad, Amin, Andrew C. Parnell, Alice Greene, Peter Thorne, Brian P. Kelleher, Robert J. N. Devoy, and Gerard McCarthy. 2022. “A Newly Reconciled Dataset for Identifying Sea Level Rise and Variability in Dublin Bay.” Ocean Science 18 (2): 511–22. https://doi.org/10.5194/os-18-511-2022.\n\n\nSkofronick-Jackson, Gail, Wesley Berg, Chris Kidd, Dalia B. Kirschbaum, Walter A. Petersen, George J. Huffman, and Yukari N. Takayabu. 2018. “Global Precipitation Measurement (GPM): Unified Precipitation Estimation from Space.” In, edited by Constantin Andronache, 175–93. Springer Remote Sensing/Photogrammetry. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-72583-3_7.\n\n\nSouth Dublin City Council. n.d. “Sustainable Drainage Systems (SuDS).” https://www.sdcc.ie/en/services/environment/environmental-health/water-services/sustainable-drainage-systems/https:/www.sdcc.ie/en/services/environment/environmental-health/water-services/sustainable-drainage-systems/sustainable-drainage-systems-suds.html.\n\n\nSridharan, Vamsi Krishna, Saurav Kumar, and Swetha Madhur Kumar. 2022. “Can Remote Sensing Fill the United States’ Monitoring Gap for Watershed Management?” Water 14 (13): 1985. https://doi.org/10.3390/w14131985.\n\n\nWhitehurst, Daniel, Kunal Joshi, Kevin Kochersberger, and James Weeks. 2022. “Post-Flood Analysis for Damage and Restoration Assessment Using Drone Imagery.” Remote Sensing 14 (19): 4952. https://doi.org/10.3390/rs14194952."
  },
  {
    "objectID": "Week_3.html#reference",
    "href": "Week_3.html#reference",
    "title": "3  Week_3",
    "section": "3.4 Reference",
    "text": "3.4 Reference\n\n\n\n\nDeng, Jiqiu, Wuzhou Dong, Yiwei Guo, Xiaoyan Chen, Renhao Zhou, and Wenyi Liu. 2023. “A Novel Remote Sensing Image Enhancement Method, the Pseudo-Tasseled Cap Transformation: Taking Buildings and Roads in GF-2 as an Example.” Applied Sciences 13 (11): 6585. https://doi.org/10.3390/app13116585.\n\n\nDuan, YL, L. Zhang, L. Yan, Taixia Wu, Yan Liu, and Qiuping Tong. 2014. “Relative Radiometric Correction Methods for Remote Sensing Images and Their Applicability Analysis.” Journal of Remote Sensing 18 (January): 597–617. https://doi.org/10.11834/jrs.20143204.\n\n\nJonah, Iyowuna Benjamin, and Taripredo Moses Aketi. n.d. “Atmospheric Correction of Landsat Image | Journal of Environmental and Geographical Studies.” https://gprjournals.org/journals/index.php/JEGS/article/view/120.\n\n\nMuchsin, Fadila, Kuncoro Adi Pradono, Indah Prasasti, Dianovita Dianovita, Kurnia Ulfa, Kiki Winda Veronica, Dandy Aditya Novresiandi, and Andi Ibrahim. 2023. “EFFECT OF ATMOSPHERIC CORRECTION ALGORITHM ON LANDSAT-8 AND SENTINEL-2 CLASSIFICATION ACCURACY IN PADDY FIELD AREA.” International Journal of Remote Sensing and Earth Sciences (IJReSES) 20 (1): 57–65. https://doi.org/10.30536/j.ijreses.2023.v20.a3845.\n\n\nÖzciḣan, Buğrahan, Levent Doğukan Özlü, Mümin İlker Karakap, Halime Sürmeli̇, Ugur Alganci, and Elif Sertel. 2023. “A Comprehensive Analysis of Different Geometric Correction Methods for the Pleiades -1A and Spot-6 Satellite Images.” International Journal of Engineering and Geosciences 8 (2): 146–53. https://doi.org/10.26833/ijeg.1086861.\n\n\nShah, Maitrik, Mehul S Raval, and Srikrishnan Divakaran. 2022. “IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium.” In, 346–49. https://doi.org/10.1109/IGARSS46834.2022.9884900.\n\n\nTarasenkov, M. V., A. V. Zimovaya, V. V. Belov, and M. V. Engel. 2019. “25th International Symposium on Atmospheric and Ocean Optics: Atmospheric Physics.” In, 11208:197–202. SPIE. https://doi.org/10.1117/12.2539123.\n\n\nWang, Yu, Zhenfeng Shao, Tao Lu, Changzhi Wu, and Jiaming Wang. 2023. “Remote Sensing Image Super-Resolution via Multiscale Enhancement Network.” IEEE Geoscience and Remote Sensing Letters 20: 1–5. https://doi.org/10.1109/LGRS.2023.3248069.\n\n\nYan, Shi, Liuwei Sheng, Gu Chao, Liu Hui, Zhang Yang, and Huangchun Hao. 2023. “Multi-Source Satellite Remote Sensing Image Processing and Processing Method Based on Geometric Correction Model.” In, edited by Roumen Kountchev, Kazumi Nakamatsu, Wenfeng Wang, and Roumiana Kountcheva, 293–303. Smart Innovation, Systems and Technologies. Singapore: Springer Nature. https://doi.org/10.1007/978-981-19-7184-6_26."
  },
  {
    "objectID": "Week_1.html#application",
    "href": "Week_1.html#application",
    "title": "1  Week_1",
    "section": "1.2 Application",
    "text": "1.2 Application\n\nThroughout the week, remote sensing technology is a method of observing and analysing the Earth and its atmosphere from a distance using radio waves or other forms of electromagnetic waves. This technology enables imaging and analysis of distant targets in a non-contact manner by collecting and processing electromagnetic wave information from the Earth’s surface and atmosphere. With the development of technology, remote sensing has become an important tool in the field of geographic information science and is widely used in a number of research and practical applications(Q. Zhou 2023). In the field of urban environmental monitoring, remote sensing technology provides an efficient means of monitoring air quality, water quality and land use. Urban sprawl, traffic flow and pollution from industrial activities can be observed and analysed from the air through remote sensing technology, which helps in decision-making for urban planning and environmental protection(J. Zhou 2023).In the area of vegetation monitoring, remote sensing can be used to analyse the health, growth dynamics and coverage of plants, and by analysing reflected and absorbed electromagnetic waves of specific wavelengths, scientists are able to assess important parameters such as the biomass, water content and chlorophyll concentration of vegetation. This information is important for the optimisation of agricultural production, forestry management and ecological conservation(Adhikary et al. 2022a).Geological research has also benefited from the application of remote sensing technology, and by analysing the electromagnetic wave reflection characteristics of the earth’s surface, scientists have been able to identify different rock types, the distribution of mineral deposits and changes in topography and geomorphology. This is of great value for the exploration of mineral resources, the early warning of geological disasters and the monitoring of environmental changes(Torres Gil, Valdelamar Martínez, and Saba 2023).Agriculture is one of the areas in which remote sensing technology is most widely used. In addition to being used for assessing plant health and yield estimation, remote sensing is used to monitor irrigation needs, detect weeds and pests and make weather forecasts. This makes agricultural production more precise and efficient, helping to increase crop yields and reduce resource wastage(Adhikary et al. 2022b).\nIn addition, synthetic aperture radar (SAR) technology, as an advanced remote sensing technology, has a wide range of applications. By using radar waves to penetrate the limitations of cloud cover and lighting conditions, it provides a unique perspective for surface observation. This technology is capable of acquiring high-quality image data under any weather conditions, including dense fog, cloud cover, rain and snow, as well as at night. As a result, SAR has become an important tool for geological exploration, topographic mapping, disaster monitoring and prevention, traffic surveillance, and agricultural and forest monitoring, among other fields.(Hu et al. 2022)。\nSAR systems work by transmitting and receiving microwave signals, using the returned signals to calculate the position, shape and other characteristics of objects on the ground. A key advantage of this active sensing technology is its sensitivity to the type of ground material and moisture content, allowing it to play an important role in monitoring vegetation-covered areas, urban environments and bodies of water, where SAR imagery is unique in analysing surface changes, assessing the impacts of disasters and carrying out environmental monitoring, owing to its unique imaging properties(Navalgund, Jayaraman, and Roy 2007)."
  },
  {
    "objectID": "Week_1.html#reflecting",
    "href": "Week_1.html#reflecting",
    "title": "1  Week_1",
    "section": "1.3 Reflecting",
    "text": "1.3 Reflecting\n\nReflecting on the week’s learning, I am struck by the profound impact that remote sensing has on our understanding of the world. The ability to observe and analyse our planet from a distance provides a unique perspective that is not possible through ground-based observations alone. It literally allows us to see the bigger picture, providing a comprehensive understanding of large-scale environmental patterns, urban development and even climate change.\nRemote sensing is interdisciplinary in nature. It’s not just a tool for geographers or environmental scientists; it integrates physics, engineering, environmental science and even policy-making. This integration demonstrates the collaborative effort required to address complex global issues. For example, remote sensing data can inform policies on urban planning, agricultural practices and disaster management, making it a critical tool for sustainable development.\nAnother aspect is the rapid evolution of remote sensing technology. With innovations such as high-resolution imagery and real-time data analysis, the potential applications of this technology are expanding at an incredible rate."
  },
  {
    "objectID": "Week_1.html#reflection",
    "href": "Week_1.html#reflection",
    "title": "1  Week_1",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\n\nReflecting on the week’s learning, the profound impact of remote sensing on our understanding of the world is truly remarkable. This technology, with its ability to observe and analyse our planet from afar, offers a unique perspective that ground-based observations simply cannot match. It’s like having a bird’s-eye view of the world, which opens up a whole new dimension in comprehending large-scale environmental patterns, urban sprawl, and the intricate dynamics of climate change.\nWhat strikes me the most is how remote sensing serves as a bridge across various disciplines. It’s not confined to the realm of geography or environmental science; rather, it embodies a synergy of physics, engineering, environmental science, and even policy-making. This multidisciplinary approach highlights the collective effort needed to tackle the complex challenges our world faces today. For instance, the data derived from remote sensing technologies play a pivotal role in shaping policies related to urban planning, optimizing agricultural practices, and enhancing disaster management strategies. It underscores the technology’s indispensability in driving sustainable development forward.\nThe rapid advancement in remote sensing technologies is nothing short of astonishing. Innovations such as high-resolution imagery and the capability for real-time data analysis are broadening the horizons of its applications at an unprecedented pace. These advancements are not only improving our current capabilities but are also paving the way for new possibilities in monitoring and managing the Earth’s resources more effectively. This week’s exploration into the world of remote sensing has deepened my appreciation for this powerful tool and its potential to contribute to a more sustainable and informed future."
  },
  {
    "objectID": "Week_4.html#reflection",
    "href": "Week_4.html#reflection",
    "title": "4  Week_4",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\n\nIncorporating the application of remote sensing technologies into the context of the Dublin City Development Plan 2016-2022 and its Strategic Flood Risk Assessment (SFRA) not only broadens the scope of my reflection but also enhances the depth of analysis on how technological advancements can be synergized with urban planning and environmental management practices. This expanded consideration allows for a multifaceted approach towards sustainable urban development, emphasizing the critical role of innovative technologies in addressing complex urban challenges.\nRemote sensing, with its capacity to collect detailed environmental data from a distance, presents an invaluable tool for urban planners and policymakers. By facilitating a comprehensive analysis of land use changes, vegetation cover, water bodies, and urban infrastructure, remote sensing data can significantly contribute to informed decision-making processes. For Dublin, leveraging such technologies means the ability to dynamically monitor urban expansion and its impacts on flood risks, assess vulnerabilities across the urban landscape, and develop targeted strategies for flood mitigation and urban resilience.\nThis integration goes beyond traditional planning methods by enabling a proactive rather than reactive approach to urban development and environmental stewardship. The precision and timeliness of data provided by remote sensing can lead to the early identification of potential flood-prone areas, changes in land cover that may affect hydrological cycles, and the effectiveness of existing flood defense structures. Consequently, this facilitates the optimization of land use planning, infrastructure development, and environmental conservation efforts to mitigate flood risks effectively.\nFurthermore, the application of remote sensing technologies underscores the importance of interdisciplinary collaboration and capacity building among urban planners, environmental scientists, and the broader community. Engaging with a diverse range of stakeholders in the interpretation and application of remote sensing data can foster a more inclusive and participatory approach to urban planning. This collaborative framework not only enhances the understanding and management of flood risks but also promotes a shared sense of responsibility and collective action towards sustainable urban development. Moreover, the ongoing advancement in remote sensing technologies, including higher resolution imagery, real-time data acquisition, and improved analytical tools, offers new opportunities for innovation in urban planning and environmental management. As cities like Dublin strive to align their development strategies with global sustainability goals, the integration of such technologies becomes increasingly crucial. It allows for a more nuanced understanding of urban ecosystems, the interconnections between human activities and natural processes, and the pathways towards achieving a harmonious balance between urban development and environmental preservation.\nIn summary, the reflection on integrating remote sensing technologies into Dublin’s urban planning and flood risk management efforts highlights the transformative potential of such tools in advancing sustainable urban development. It points towards a future where technology and data-driven insights become central to crafting resilient, inclusive, and sustainable urban landscapes. This expanded perspective not only enriches my understanding of the complexities involved in urban planning but also inspires a forward-looking approach to leveraging technology for the betterment of our urban environments."
  },
  {
    "objectID": "Week_6.html#knowledge-gain-from-the-lecture",
    "href": "Week_6.html#knowledge-gain-from-the-lecture",
    "title": "5  Week_6",
    "section": "5.1 Knowledge gain From the Lecture",
    "text": "5.1 Knowledge gain From the Lecture\nGoogle Earth Engine是Google提供的对大量全球尺度地球科学资料（尤其是卫星数据）进行在线可视化计算和分析处理的云平台。它的特点是允许大规模的地理空间分析，运行速度很快，有在客户端运行的代码，并且将数据储存在的服务器上.\nGEE 中的栅格数据和矢量数据:\n\n栅格数据称为”Image”，具有波段（bands）。\n矢量数据称为”Feature”，具有几何形状和属性（dictionary of properties）。\n\nGEE 中的图像缩放:\n\n指图像的分辨率，即每个像素代表的实际地面距离。\nGEE 会根据分析需求自动选择合适的缩放级别。\n\nGEE中的投影:\nGEE 支持多种投影，包括 Mercator 投影、Albers 投影、等距圆柱投影等。用户可以选择合适的投影进行分析。\n如何使用GEE\n\n\n\n\n\nHow to use GEE, Source: Google Earth Engine\n\n\n\n\nGEE中可以进行的操作：\n\n几何操作：例如空间操作，连接（Joins），区域统计（比如邻域的平均温度），图像或特定值的筛选。\n机器学习，包括监督学习和非监督学习，使用TensorFlow进行深度学习，探索变量之间的关系。\n应用/输出：在线图表，可扩展的使用GEE数据的地理空间应用\n\nGEE中的Reduce the Image :\nReducing the image by region 和 reducing the image by neighbor 都是用于对图像进行区域化或邻域化操作的函数。它们的主要区别在于：\n\nReducing the image by region 是根据指定的区域对图像进行操作。每个区域可以是任意形状，可以重叠。该函数会对每个区域内的所有像素进行指定的计算，并返回一个包含区域统计结果的新图像。\nReducing the image by neighbor 是根据指定的邻域对图像进行操作。每个像素的邻域是指其周围一定范围内的像素。该函数会对每个像素及其邻域内的所有像素进行指定的计算，并返回一个包含像素新值的图像。\n\nGEE中的Linear Regression & Join (暂时写不出来)"
  },
  {
    "objectID": "Week_6.html#practical",
    "href": "Week_6.html#practical",
    "title": "5  Week_6",
    "section": "5.2 Practical",
    "text": "5.2 Practical\n\n5.2.1 Compare with RGB & NDVI\n\n\n\n\n\nCompare with RGB & NDVI Slide the slider to see the changes in RGB and NDVI.\n\n\n5.2.2 PCA Analysis\nSelecting a region, it first normalizes the image bands, calculates the covariance matrix, extracts the eigenvalues and eigenvectors, and then projects the image data into the principal component space. Finally, it displays the first principal component of the PCA result on the map.\n\n\n\n\n\nPCA Analysis"
  },
  {
    "objectID": "Week_6.html#applications",
    "href": "Week_6.html#applications",
    "title": "5  Week_6",
    "section": "5.3 Applications",
    "text": "5.3 Applications\n\n5.3.1 General Applications\nGEE has been the focus of attention in remote sensing big data processing.GEE is a cloud-based platform for parallel processing of geospatial data globally using Google’s cloud.GEE is a free cloud-based platform hosting more than 40 years of petabyte-sized remote sensing data, such as Landsat, MODIS, National Oceanic and Atmospheric Administration’s Advanced Ultra-High Resolution Radiometer (NOAA AVHRR), Sentinel 1, 2, 3, and 5-P; and Advanced Land Observing Satellite (ALOS) data(Aghamiri et al., n.d.). GEE also includes climate-weather and geophysical datasets. Other off-the-shelf products such as the Enhanced Vegetation Index (EVI) and the Normalised Vegetation Index (NDVI) are also available. In addition to having access to a large repository of raw remote sensing imagery, users can access pre-processed imagery, cloud removal imagery and mosaics in the GEE data catalogue(Ritika, n.d.).\n\n\n\n\n\nA summary of the algorithms and capabilities available in code editor-Google Earth Engine, Author: (Tamiminia et al. 2020)\n\n\n\n\nHere are some applications of GEE in practice.\n\n\n\n\n\nGEE in Different Applications, Author: (Tamiminia et al. 2020)\n\n\n\n\n\n\n5.3.2 GEE and data types\nMuch of the academic use of GEE has been optical, thanks in large part to the 40-year free archive of Landsat imagery, and optical remote sensing data is still the most commonly used data source.The GEE data catalogue provides optical satellite imagery from 1972 to the present day, allowing researchers to conduct Earth monitoring studies(Pham-Duc et al. 2023). In addition, the wide range of applications by GEE users shows that optical images are easier to process and interpret for non-remote sensing experts. This is one of the reasons why GEE has a global reach in a wide range of scientific fields. At the same time, the combination of SAR and optical satellite data can help researchers to solve problems such as cloud cover. Especially in the tropics, the efficacy of optical images can be greatly affected due to continuous cloud cover and situations such as forest fires. Therefore, combining optical and SAR data can improve the accuracy of classification and provide more information to monitor surface changes.(Lea et al. 2023).\n\n\n5.3.3 GEE and sensor type\nLandsat is considered an important source of remote sensing data in the GEE, as it provides a continuous image of the Earth’s surface.The Landsat 9 satellite will be launched in 2020 with the aim of continuing the Landsat programme’s key role in monitoring the Earth’s resources. Long-term land cover change studies can be carried out on a regional and global scale(Pham-Duc et al. 2023).\nSentinel-1 is another popular source of satellite imagery and has achieved very accurate classification results.Sentinel-1 consists of two satellites, Sentinel-1A and Sentinel 1-B, which were launched in 2014 and 2016, respectively, with a spatial resolution of 10 m and a revisit time of 6 days(Arias Cuenca 2023). It is equipped with a dual-polarised C-band SAR sensor that provides data in all-weather, day and night conditions.The Sentinel-2 mission consists of a constellation of two satellites, Sentinel-2A (launched in 2015) and Sentinel-2B (launched in 2017), which provide spatially resolved 10 m, 20 m and 60 m optical images, and temporal resolution of about 5 days(Xu, Heremans, and Somers 2022; Lechner et al. 2022).\nThe Moderate Resolution Imaging Spectroradiometer (MODIS) was launched on the Terra satellite in 1999 and on the Aqua satellite in 2002. Researchers can access MODIS data in GEE in 36 spectral bands and at three varying spatial resolutions (250 m, 500 m, and 1 km) at 1-day revisit times. Even though MODIS has a low spatial resolution, its high temporal resolution allows researchers to monitor short- and long-term global environmental change (dynamics)(Wu and Xiong 2020).\n\n\n5.3.4 Remote sensing data analysis\n\n5.3.4.1 Machine learning techniques\nMachine learning is a subset of artificial intelligence that deals with the design of algorithms to train models to make decisions or predictions. Machine learning methods can be divided into two broad categories: parametric and non-parametric. Parametric machine learning algorithms use a fixed number of parameters or assumptions. Machine learning methods have been effectively used for remote sensing data processing. Classification, clustering, regression and dimensionality reduction are the four main categories of analysis for machine learning algorithms(Shaveta 2023). Regression is a supervised machine learning method designed to estimate or predict output variables based on a set of covariates. Another positive aspect of linear regression is its fast computational speed, which is an important factor in geographic big data analysis(“Google Earth Engine and Machine Learning for Earth Monitoring,” n.d.). Also the MLR model deals with the non-linear relationship between the dependent and independent variables in the prediction process.\n\n\n5.3.4.2 Other GEE image processing functions\nGEE provides a wide range of image processing tools suitable for the analysis of remotely sensed data. These tools cover time series analysis, feature extraction, colour compositing of images and image preprocessing, mainly for satellite images, rather than machine learning-based methods(“Google Earth Engine and Machine Learning for Earth Monitoring,” n.d.). With the rapid changes in the Earth’s surface, time-series analysis of satellite imagery has become critical, helping to track trends, monitor changes and develop predictive models.GEE has been used for such analyses in a number of studies because of its ability to handle high resolution or large amounts of data, and is particularly good at monitoring changes in the land surface. Feature extraction techniques, by analysing spectral and geometric attributes of images, help to identify regional relationships in images, which is critical for resource conservation and information retention(Pham-Duc et al. 2023). In addition, GEE’s visual interpretation tools are widely used in land monitoring studies to extract key information from colour composite images. In addition, GEE’s image pre-processing capabilities support image mosaicing, cloud processing and error detection, and although cloud coverage is a major challenge when working with optical data, GEE provides effective tools and algorithms to support these tasks(Lea et al. 2023).\nOn the algorithmic side, there are challenges in implementing new algorithms, especially deep learning models. Although GEE has recently established a connection to TensorFlow, enabling users to interact with models stored on the Google AI platform, it does not directly support deep learning classifiers. Most research has used pixel-based classification methods and there is a need for better support for complex unsupervised classification algorithms as well as improvements in object-based image analysis, heavy vector manipulation, and the availability of high-resolution satellite imagery(Nia et al., n.d.)."
  },
  {
    "objectID": "Week_6.html#reflection",
    "href": "Week_6.html#reflection",
    "title": "5  Week_6",
    "section": "5.4 Reflection",
    "text": "5.4 Reflection\nReflecting on the comprehensive insights gained from the past week’s deep dive into Google Earth Engine (GEE), my intellectual journey through the realm of remote sensing has been both illuminating and transformative. The exploration extended beyond the fundamental technicalities of GEE’s raster and vector data handling; it ventured into the practical applications and profound implications of this powerful tool in understanding and interacting with our planet.\nThe nuanced understanding of image reduction techniques—distinguishing between region-based and neighbor-based reductions—was particularly revelatory. It highlighted GEE’s nuanced capacity to distill vast amounts of geospatial data into actionable insights. This aspect of GEE not only enhances its analytical precision but also expands its utility across various research and operational contexts, enabling a more targeted and efficient analysis of environmental phenomena.\nThe enlightening exploration of GEE’s vast capabilities has significantly broadened my perspective, revealing the interdisciplinary potential of remote sensing technology. It’s fascinating to see how the integration of various data types—from climate variables to topographical features—can offer a more comprehensive understanding of environmental dynamics. This integrative approach not only enhances the accuracy of environmental assessments but also fosters a more holistic understanding of our planet’s intricate systems.\nThe reflective journey through this week has been a blend of learning, discovery, and inspiration. It has fortified my appreciation for the critical role of remote sensing in environmental science and its potential to inform and inspire sustainable interventions. The insights gained are not merely academic; they resonate with the pressing need for innovative solutions to global environmental challenges. As I look forward to the continuation of this academic journey, I am motivated by the potential to contribute meaningfully to our collective understanding and stewardship of the Earth, leveraging the powerful capabilities of GEE to illuminate the path toward a sustainable future.\nThis experiential learning has not only enriched my technical proficiency but also deepened my commitment to leveraging technology in pursuit of environmental sustainability. The profound capabilities of GEE—to process, analyze, and visualize complex geospatial datasets—offer a compelling glimpse into the future of environmental monitoring and analysis. Embracing this technology’s potential can revolutionize our approach to confronting and mitigating the pressing environmental challenges of our time."
  },
  {
    "objectID": "Week_6.html#reference",
    "href": "Week_6.html#reference",
    "title": "5  Week_6",
    "section": "5.5 Reference",
    "text": "5.5 Reference\n\n\n\n\nAghamiri, Mahtab, Amineh Ghorbani, Jolien Ubacht, Igor Nikolic, and Paulein Herder. n.d. “Enabling Citizen Participation in Sustainable Collec- Tive Action In Smart Cities: The Case Of Buiksloter- Ham.”\n\n\nArias Cuenca, María. 2023. “Sentinel-1 time series applications over agricultural fields: proposal, evaluation and comparison of different methodologies.” https://doi.org/10.48035/Tesis/2454/45156.\n\n\n“Google Earth Engine and Machine Learning for Earth Monitoring.” n.d. https://pos.sissa.it/429/021.\n\n\nLea, James, Robert Fitt, Stephen Brough, Georgia Carr, Jonathan Dick, Natasha Jones, Eli Saetnan, and Richard Webster. 2023. “Google Earth Engine Climate Tool (GEEClimT): Enabling Rapid, Easy Access to Global Climate Reanalysis Data.” https://doi.org/10.5194/egusphere-egu23-7760.\n\n\nLechner, Michael, Alena Dostálová, Markus Hollaus, Clement Atzberger, and Markus Immitzer. 2022. “Combination of Sentinel-1 and Sentinel-2 Data for Tree Species Classification in a Central European Biosphere Reserve.” Remote Sensing 14 (11): 2687. https://doi.org/10.3390/rs14112687.\n\n\nNia, Vahid Partovi, Guojun Zhang, Ivan Kobyzev, Michael R. Metel, Xinlin Li, Ke Sun, Sobhan Hemati, et al. n.d. “Mathematical Challenges in Deep Learning.” https://doi.org/10.48550/arXiv.2303.15464.\n\n\nPham-Duc, Binh, Ho Nguyen, Hien Phan, and Quan Tran-Anh. 2023. “Trends and Applications of Google Earth Engine in Remote Sensing and Earth Science Research: A Bibliometric Analysis Using Scopus Database.” Earth Science Informatics 16 (3): 2355–71. https://doi.org/10.1007/s12145-023-01035-2.\n\n\nRitika, Prasai. n.d. “Earth Engine Application to Retrieve Long-Term Terrestrial and Aquatic Time Series of Satellite Reflectance Data.” International Journal of Multidisciplinary Research and Growth Evaluation.\n\n\nShaveta. 2023. “A Review on Machine Learning.” International Journal of Science and Research Archive 9 (1): 281–85. https://doi.org/10.30574/ijsra.2023.9.1.0410.\n\n\nTamiminia, Haifa, Bahram Salehi, Masoud Mahdianpari, Lindi Quackenbush, Sarina Adeli, and B. Brisco. 2020. “Google Earth Engine for Geo-Big Data Applications: A Meta-Analysis and Systematic Review.” ISPRS Journal of Photogrammetry and Remote Sensing, May. https://doi.org/10.1016/j.isprsjprs.2020.04.001.\n\n\nWu, A., and X. Xiong. 2020. “Sensors, Systems, and Next-Generation Satellites XXIV.” In, 11530:267–77. SPIE. https://doi.org/10.1117/12.2573018.\n\n\nXu, Fei, Stien Heremans, and Ben Somers. 2022. “Urban Land Cover Mapping with Sentinel-2: A Spectro-Spatio-Temporal Analysis.” Urban Informatics 1 (1): 8. https://doi.org/10.1007/s44212-022-00008-y."
  },
  {
    "objectID": "Week_7.html#knowledge-gain-from-the-lecture",
    "href": "Week_7.html#knowledge-gain-from-the-lecture",
    "title": "6  Week_7",
    "section": "6.1 Knowledge gain From the Lecture",
    "text": "6.1 Knowledge gain From the Lecture\nClassification and regression trees (CART)\n\nClassification\n\n分类树用于将数据分类到两个或更多的离散类别 回归树处理线性回归不适用的情况 通过将数据分割成小块来改进模型的预测能力 在创建决策树时，最终的叶子节点可能是类别的混合（不纯），并使用基尼不纯度来量化这种不纯度。选择最低不纯度的属性作为树的顶部来开始决策过程。 计算基尼不纯度，并用它来评估在构建决策树时分割数据的质量，其值越小表示数据越纯净。\n\n\n\n\n\nLand Cover Classification using Google Earth Engine and Random Forest Classifier—The Role of Image Composition, Source: (Phan, Kuch, and Lehnert 2020)\n\n\n\n\n\nRegression trees\n\n回归树预测连续值，例如污染量，而分类树预测离散值，例如土地覆盖类型。 当线性回归不能很好地拟合数据时，建议使用回归树作为替代方案。在回归树中，数据根据阈值或节点划分为多个部分。计算这些部分的残差平方和（SSR），并且具有最低SSR的阈值成为树的起点或根。可以重复该过程以进一步分割数据，并且可以设置最小观察次数以防止过度拟合。\n\n\n\n\n\nTree classification procedure in Google Earth Engine, Source: (Laengner, Siteur, and Wal 2019)\n\n\n\n\nOverfitting\n如果一个叶节点只包含一个人或一个像素值，就可能出现过拟合。最好的模型具有低偏差和低变异性，能够在不同数据集（如训练集和测试集）之间做出一致的预测。为了防止决策树过度生长的方法，其方法包括限制树的生长（例如，一个叶子至少包含20个像素），以及最弱连接剪枝（基于树得分的剪枝）。\n每棵树的叶子数量和调整α值（正则化参数）来减少过拟合。从α=0开始，逐渐增加α值直到剪枝可以降低树得分，然后保存这些α值。树得分是残差平方和（SSR）加上树的惩罚（α乘以叶子数T）。不同的α值会产生不同的子树和树得分。使用不同的α值来训练数据，并在测试数据上计算SSR，以选择得分最小的树。用交叉验证（10次交叉验证）来重复上述过程，从而找到平均而言在测试数据上SSR最低的α值。然后选择这个α值对应的、使用全部数据训练的树。对于分类树，SSR将被不纯度度量（如基尼不纯度）所替代。\nRandom Tress\n随机森林由许多分类决策树组成，通过对数据进行自助采样（bootstrap samples），并从随机选择的变量中构建决策树。在节点上，算法会再次从变量的随机子集中选择。这个过程会不断重复，最终得到多棵树，即一个”森林”。当有新数据通过这些树时，每棵树都会给出一个预测结果，最终选择票数最多的选项作为最终预测。\n随机森林中的”bagging”技术，即通过替换数据进行自助采样。每棵树大约使用70%的训练数据进行训练，剩下30%的数据被称为袋外数据（OOB）。袋外数据被用来测试森林，以评估模型的性能，最后选择得票最多的分类结果。袋外数据分类错误的比例被称为OOB错误。\n随机森林中不进行剪枝，树可以尽可能地生长。袋外错误是通过计算没有使用某些值（例如数据中的行）的所有树的平均预测错误来得出的。验证数据与袋外数据不同，它从未被包含在决策树的构建中。\nHow to apply to the imagery\n图像分类的两种主要方法：监督学习和无监督学习。监督学习通过机器学习模式识别从数据中学习并对新数据打标签，而无监督学习则通过聚类分析未预先定义的数据，然后对这些聚类进行标签。\n监督学习：\n\n监督学习的通用基本上都遵循流程包括：类别定义、预处理、训练、像素分配和准确性评估。\n\n无监督学习:\n\nDBSCAN算法，它通过设定一个半径（Epsilon）和最小点数来形成聚类，并可通过迭代和PCA进行优化。\nISODATA算法，k-means的一个变体，它增加了合并过近的聚类或分割过长的聚类的功能，并根据聚类中的像素数、迭代次数等条件来控制聚类过程.\n“Cluster busting”的方法，它通过掩盖和重新分类那些难以打标签或标签不正确的聚类来提高分类精度.\n\nMaximum likelihood\nMaximum likelihood & Support Vector Machine 最大似然估计（Maximum Likelihood Estimation，MLE）是一种统计方法，用于估计概率模型中的参数。该方法的基本思想是：从所有可能的参数值中，选择最能解释观察到的数据的参数值。例如在遥感中，它使用概率来将图像中的每个像素分配给最可能的土地覆盖类型，并可以设置概率阈值来决定是否进行分类。\n\n\n\n\n\nMaximum likelihood classifier, Source: (Núñez et al. 2019)\n\n\n\n\nSupport Vector Machine\n支持向量机（SVM）是一种监督学习模型，用于分类和回归分析。假设我们有一个训练数据集，其中每个数据点都属于两个类别中的一个。SVM 的目标是找到一个超平面，使得该超平面能够将两类数据点尽可能分开。\n\n\n\n\n\nSVM example of linearly separable data, Source: (Sheykhmousa and Mahdianpari 2020)"
  },
  {
    "objectID": "Week_7.html#practical",
    "href": "Week_7.html#practical",
    "title": "6  Week_7",
    "section": "6.2 Practical",
    "text": "6.2 Practical\n\n6.2.1 Supervised Classification\nSelect a Study Area, select the training feature collections on the map, in the following figure selected forest, water,developed,herbaceous as the collect feature.Use ee.Classifier.smileCart) and train it. But the result is not very good, maybe the initial data set selection is not very good.\n\n\n\n\n\nSupervised trained Classification Result\n\n\n\n\n\n\n6.2.2 Unsupervised Classification\nSame result of Unsupervised trained Classification\n\n\n\n\n\nUnsupervised trained Classification Result"
  },
  {
    "objectID": "Week_7.html#application",
    "href": "Week_7.html#application",
    "title": "6  Week_7",
    "section": "6.3 Application",
    "text": "6.3 Application\nThe roots of remote sensing machine learning can be traced back to the 1990s. It was initially introduced as an approach to remote sensing for automated knowledge infrastructure building. Since then it has evolved and found applications in a variety of fields, including remote sensing and geoscience(Challa, Sridhar, and Shyam Mohan 2022). Machine learning algorithms such as deep learning are popular in remote sensing due to their ability to analyse large amounts of data and achieve high accuracy(Jeon 2023). These algorithms have been used for tasks such as image classification, scene understanding and material recognition(Rewhel et al. 2023). The availability of datasets with domain-specific attributes further facilitates the application of machine learning techniques in remote sensing.\nThere are three main categories of machine learning algorithms. One is supervised machine learning, second is unsupervised machine learning and third is reinforcement learning. The difference between supervised and unsupervised is that using supervised algorithms, there is a dataset containing output columns whereas in using unsupervised algorithms, there is only a huge dataset and its duty is to cluster the algorithms based on the relationship of the dataset to a variety of different classes between the different records that have been identified[Ling (2023)](Raju et al. 2023).\nRandom Forest algorithms are becoming increasingly popular in the remote sensing community due to their classification accuracy. These are integrated classifiers, which basically means that they utilise multiple decision trees underneath.One of the main reasons for the popularity of RF classifiers is that they help to mitigate high-dimensional problems(Bahrami, Hassani, and Maghsoudi 2018).They provide a variable importance (VI) that can reduce the dimensionality of hyperspectral data. Variable importance is essentially a measure of how a change in a particular input affects the output[Solorio-Ramírez et al. (2023)](Rina et al. 2023).\nSVMs are supervised learning models that can be used for regression and classification problems. They are mainly used for classification problems. They work by plotting points (features) in an n-dimensional space (features) and then dividing these points with a hyperplane. SVMs are used for almost all types of classification problems in remote sensing from forest classification to multispectral remote sensing image segmentation(Feizi and Nazemi 2022). Like other algorithms, their success depends on the nature of the problem, and one must test each algorithm separately and then make a decision based on each algorithm’s performance(Hazra et al. 2021).\nOverfitting a model usually requires building an overly complex model to account for characteristics and outliers in the study data. This means that if you evaluate the model with the same type of data (the type of data it has been trained on), you will get a very high prediction, classification, and classification accuracy(Schmidt, n.d.). However, if you just modify some inputs, (which the model hasn’t seen before), then the prediction, classification accuracy will drop. You can fix the overfitting by using a larger dataset and splitting the dataset appropriately. Also, it is useful to reduce the complexity of the model definition so that all extreme boundary cases are not classified(Rezaei and Sabokrou, n.d.)."
  },
  {
    "objectID": "Week_7.html#reflection",
    "href": "Week_7.html#reflection",
    "title": "6  Week_7",
    "section": "6.4 Reflection",
    "text": "6.4 Reflection\n这节课主要学习了一些机器学习的技术在遥感中的应用，讲述了使用机器学习来解决什么样的问题。在上面所陈述的方法里面，哪个才是最适用的呢？这个问题的答案取决于一个人想要解决的问题。在某些情况下，当您有多个维度但记录有限时，SVM可能会更好地工作。如果你有很多的记录，但很少的维度(特性),神经网络(NN)可能产生更好的预测/分类精度。人们经常需要在你的数据集上测试多种算法，然后选择最有效的算法。通常，需要为不同的算法调整各种参数(i)。对射频、隐藏层数、神经网络神经元的数量以及对SVMs的”决策函数形状”等进行了研究。很多时候，将多个算法组合在一起可以获得更好的准确性，这就是所谓的合奏。还可以将SVM和神经网络、SVM和RF(可能性无穷)组合起来，以提高预测精度。再次，须测试多个合奏以选择最好的合奏。\n同样重要的是要注意,预测精度可能会改变根据特定功能试图使用分类、预测的目的而改变。例如，Shang和Chisholm(2014)讨论了如何将澳大利亚本土森林物种分类，他们决定使用最先进的遥感算法。在树叶、树冠和社区层面对树木进行分类。他们测试了各种算法(SVM、AdaBoost和Random Forest)，并发现每种算法在不同级别上都优于其他算法。在叶级，随机森林获得了最佳分类精度(94.7%)，支持向量机在冠层(84.5%)和社区水平(75.5%)的表现优于其他算法。\n另一个影响算法选择的因素是数据是否线性可分。例如，线性分类算法期望数据可以被线性空间中的直线分割。假设数据是线性可分的，可能适用于大多数情况，但在某些场景下是正确的,并会降低预测/分类精度。因此，我们需要确保使用的算法能够处理可用的数据。\n不可能只看一种算法，从理论上决定它是否会为你的数据集产生最好的结果，因为很多机器学习算法都是黑盒算法。这意味着很难看出算法是如何达到特定的结果的。因此，首先根据问题的类型来缩小算法选择的范围，然后在数据集的一部分应用缩小算法，看看哪一种性能最好。\n机器学习有着光明的未来，因为越来越多的人正在学习机器学习的基本知识，并将其应用于日常工作和研究中。新的算法每隔一天就会出现，分类的准确率也随之提高。这些问题在遥感(测绘地皮)中似乎很困难，有时甚至是不可能的，但每天都被新出现的算法解决。在不久的将来，世界上大多数的分析工作将由机器学习算法完成。"
  },
  {
    "objectID": "Week_8.html#knowledge-gain-from-the-lecture",
    "href": "Week_8.html#knowledge-gain-from-the-lecture",
    "title": "7  Week_8",
    "section": "7.1 Knowledge gain From the Lecture",
    "text": "7.1 Knowledge gain From the Lecture\nObject based image analysis (OBIA)\n这是一种考虑地面物体如何在栅格单元上表示的分析方法。 Simple Linear Iterative Clustering算法是生成超像素的最常用方法。它将图像分割成具有相似颜色和空间位置的区域，称为超像素。超像素分割可以用于图像降噪、边缘检测、纹理分析等任务。SLIC算法的基本思想是迭代地更新每个像素点的聚类标签。在每个迭代步骤中，算法会计算每个像素点与其相邻像素点的距离，并将其分配给距离最近的聚类中心。\n可能这个概念有些的抽象对于非初学者来说，我把它细致化的解释一下 想象一下，你有一张由许多像素点组成的图像。你想将这些像素点分成若干个组，使得每个组中的像素点具有相似的颜色和空间位置。\nSLIC算法就像是一个将像素点分组的”游戏”。游戏的规则如下：\n首先，你需要在图像中随机选择一些点作为”聚类中心”。 然后，你需要计算每个像素点与所有聚类中心的距离。 每个像素点将被分配给距离它最近的聚类中心。 接下来，你需要更新每个聚类中心的坐标，使其位于该聚类中所有像素点的平均位置。 重复步骤2到4，直到所有像素点都被分配给某个聚类中心。 游戏结束后，你将得到一组具有相似颜色和空间位置的像素点，即超像素。\n\n\n\n\n\nComparison of Simple Linear Iterative Clustering (SLIC) and SLICO superpixel adherence to natural image boundaries derived using initial clustering of 10 × 10 pixels., Source: (Csillik 2017)\n\n\n\n\nSub pixel analysis\n亚像素分析是指在图像的像素之间进行分析的技术。传统图像处理方法只关注每个像素的灰度值，而亚像素分析则可以利用像素之间灰度的细微差别来获取更精确的信息。其应用范围很广，包括：图像增强，边缘检测，纹理分析，目标识别.\n\n\n\n\n\nSuperpixel Generation Algorithm\n\n\n\n\n评估遥感数据分类准确度\n讲述了生产遥感数据后如何进行准确度评估，这是机器学习工作流程的一部分。其中的三个重要指标是：制图者准确率（Producer’s Accuracy）、用户准确率（User’s Accuracy）和总体准确率（Overall Accuracy），以及如何利用混淆矩阵（Confusion Matrix）来计算这些指标。\n模型预测结果正确时： 真阳性（TP）是模型正确预测阳性类别； 真阴性（TN）是模型正确预测阴性类别。 模型预测结果错误时： 假阳性（FP）是模型错误地预测为阳性，但实际为阴性； 假阴性（FN）是模型错误地预测为阴性，但实际为阳性。\n计算上面的指标是如下表\n\n\n\n\n\n\n\n\nAccuracy Metric\nFormula\nShort Definition\n\n\n\n\nProducer’s Accuracy\nTP / (TP + FN)\nCorrect classification proportion compared to ground truth.\n\n\nUser’s Accuracy\nTP / (TP + FP)\nCorrect classification proportion out of all classified.\n\n\nOverall Accuracy\n(TP + TN) / (TP + FP + FN + TN)\nProportion of all correctly classified pixels.\n\n\n\n在此基础上可以使用凯帕系数来进行有效评估分类模型的性能，确保分类结果的可靠性和准确性。凯帕系数的值范围从-1（完全不一致）到1（完全一致）。值为0表示一致性与随机机会相同，而值接近1表示非常高的一致性。其计算方法是（实际一致性 - 随机一致性）/（1 - 随机一致性）。\n\n\n\n\n\nExample of Accuracies and kappa coefficient of land use:land cover (LULC) classifications in the SMA, Source: (Priyankara et al. 2019)\n\n\n\n\nF1 Score\n当你的数据集中的类别是不平衡的，即一个类的样本数量远多于另一个类，卡帕系数将不使用，但是F1 Score特别有用。在这种情况下，单纯使用准确率可能不足以反映模型的性能，因为模型可能只是在预测主导类别方面表现良好，而忽视了少数类。F1 Score通过平衡精确率（预测为正类别中实际为正类别的比例）和召回率（实际为正类别中预测为正类别的比例），提供了一个更全面的性能度量。如果你的任务中同样重视精确率和召回率，即你希望减少假阳性和假阴性的数量，那么F1 Score是一个合适的选择。它确保了你不会因为提高另一个而牺牲了其中一个指标。尽管F1 Score可以扩展到多类分类问题，但它主要适用于二分类问题，尤其是当正负样本的重要性基本相等时。在研究二分类问题时候可以引用ROC曲线，其提供了一种在不同类别分布或不同的代价/权重条件下比较多个分类器的方法。即使在数据集随时间变化或在不同数据集上，ROC曲线和AUC值也能为模型的比较提供一致的评估标准。\nSpatial cross validation\n在传统的machine Learning交叉验证方法中，数据集被随机分成训练集和测试集。模型在训练集上进行训练，并在测试集上进行评估。然而，这种随机划分的方法忽略了遥感数据中一个关键特性——空间自相关，即相邻区域之间往往具有相似的属性。\n举一个例子现在有一张包含多种地形的大型遥感图像，比如森林、湖泊和城市区域。任务目标是创建一个计算机模型，这个模型可以查看这张图像的任何部分，并准确地告诉你那里是森林、湖泊还是城市。为了训练这个模型，你需要从图像中选取一些样本（即图像的一小部分），告诉模型这些样本分别属于哪种地形。然后，模型会学习这些样本，尝试理解不同地形的外观。\n但这里有个问题：如果你随机选择样本，那么靠得很近的样本可能会同时出现在训练数据（模型用来学习的数据）和测试数据（用来检验模型准确性的数据）中。这就像是在考试前就已经知道了部分考题，这可能会让模型看起来表现得很好，但实际上它可能并没有真正学到如何区分不同的地形，而只是记住了那些特定的样本。\n空间交叉验证就是为了解决这个问题。我们不是随机选择样本，而是将整个图像分成几个大块区域。我们可以确保某些区域仅用于训练模型，而其他区域则用于测试模型。这样，我们就可以确信，模型在评估时遇到的数据是它之前从未见过的，这有助于我们更准确地判断模型的实际性能。\n举个例子，假设你有一张包含城市、森林和湖泊的大地图。你将地图分成了东、西两部分。你用东半部分的数据训练你的模型，这意味着模型将看到并学习这一区域的城市、森林和湖泊是什么样的。然后，你用西半部分的数据测试模型，看看它是否能准确识别出它从未”见过”的地区的不同地形。通过这种方式，你可以更好地评估模型在处理新、未知区域时的实际表现能力。\n\n\n\n\n\nImportance of spatial predictor variable selection in machine learning applications – Moving from data reproduction to spatial prediction, Source: (Meyer et al. 2019)"
  },
  {
    "objectID": "Week_7.html#reference",
    "href": "Week_7.html#reference",
    "title": "6  Week_7",
    "section": "6.5 Reference",
    "text": "6.5 Reference\n\n\n\n\nBahrami, Yousef, Hossein Hassani, and Abbas Maghsoudi. 2018. “Investigating the Capabilities of Multispectral Remote Sensors Data to Map Alteration Zones in the Abhar Area, NW Iran.” Geosystem Engineering 24 (December): 1–13. https://doi.org/10.1080/12269328.2018.1557083.\n\n\nChalla, Nagendra Panini, Parupally Sridhar, and J. S. Shyam Mohan. 2022. “A Machine Learning Perspective for Remote Sensing.” In, edited by Pala Gireesh Kumar, Kolluru V. L. Subramaniam, S. Moses Santhakumar, and Neelima Satyam D., 553–59. Lecture Notes in Civil Engineering. Singapore: Springer Nature. https://doi.org/10.1007/978-981-19-0189-8_45.\n\n\nFeizi, Amir, and Alireza Nazemi. 2022. “Classifying Random Variables Based on Support Vector Machine and a Neural Network Scheme.” Journal of Experimental & Theoretical Artificial Intelligence 0 (0): 1–24. https://doi.org/10.1080/0952813X.2022.2104385.\n\n\nHazra, Simanta, Shreyasee Ghosh, Shibsankar Bala, and Debasis Chakraborty. 2021. “2021 2nd International Conference for Emerging Technology (INCET).” In, 1–6. https://doi.org/10.1109/INCET51464.2021.9456237.\n\n\nJeon, Gwanggil. 2023. “Advanced Machine Learning and Deep Learning Approaches for Remote Sensing.” Remote Sensing 15 (11): 2876. https://doi.org/10.3390/rs15112876.\n\n\nLaengner, Marieke, Koen Siteur, and Daphne Wal. 2019. “Trends in the Seaward Extent of Saltmarshes Across Europe from Long-Term Satellite Data.” Remote Sensing 11 (July): 1653. https://doi.org/10.3390/rs11141653.\n\n\nLing, Qingyang. 2023. “Machine Learning Algorithms Review.” Applied and Computational Engineering 4 (May): 91–98. https://doi.org/10.54254/2755-2721/4/20230355.\n\n\nNúñez, Juan Manuel, Sandra Medina-Fernández, F. Gerardo Ávila, and Jorge Montejano. 2019. “High-Resolution Satellite Imagery Classification for Urban Form Detection.” In, 1–9. https://doi.org/10.5772/intechopen.82729.\n\n\nPhan, Thanh Noi, Verena Kuch, and Lukas W. Lehnert. 2020. “Land Cover Classification Using Google Earth Engine and Random Forest ClassifierThe Role of Image Composition.” Remote Sensing 12 (15): 2411. https://doi.org/10.3390/rs12152411.\n\n\nRaju, G. S Bapi, Chintala Manasa, Nandikatti Durga Bhavani, Jadi Amulya, and Dangatla Shirisha. 2023. “2023 7th International Conference on Intelligent Computing and Control Systems (ICICCS).” In, 104–9. https://doi.org/10.1109/ICICCS56967.2023.10142906.\n\n\nRewhel, Ekram M., Jianqiang Li, Amal A. Hamed, Hatem M. Keshk, Amira S. Mahmoud, Sayed A. Sayed, Ehab Samir, et al. 2023. “Deep Learning Methods Used in Remote Sensing Images: A Review.” Journal of Environmental & Earth Sciences 5 (1): 33–64. https://doi.org/10.30564/jees.v5i1.5232.\n\n\nRezaei, Hossein, and Mohammad Sabokrou. n.d. “Quantifying Overfitting: Evaluating Neural Network Performance Through Analysis of Null Space.” https://doi.org/10.48550/arXiv.2305.19424.\n\n\nRina, Su, Hong Ying, Yu Shan, Wala Du, Yang Liu, Rong Li, and Dingzhu Deng. 2023. “Application of Machine Learning to Tree Species Classification Using Active and Passive Remote Sensing: A Case Study of the Duraer Forestry Zone.” Remote Sensing 15 (10): 2596. https://doi.org/10.3390/rs15102596.\n\n\nSchmidt, James. n.d. “Testing for Overfitting.” https://doi.org/10.48550/arXiv.2305.05792.\n\n\nSheykhmousa, Reza M, and Masoud Mahdianpari. 2020. “Support Vector Machine Vs. Random Forest for Remote Sensing Image Classification: A Meta-Analysis and Systematic Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, October. https://doi.org/10.1109/JSTARS.2020.3026724.\n\n\nSolorio-Ramírez, José-Luis, Raúl Jiménez-Cruz, Yenny Villuendas-Rey, and Cornelio Yáñez-Márquez. 2023. “Random Forest Algorithm for the Classification of Spectral Data of Astronomical Objects.” Algorithms 16 (6): 293. https://doi.org/10.3390/a16060293."
  },
  {
    "objectID": "Week_8.html#practical",
    "href": "Week_8.html#practical",
    "title": "7  Week_8",
    "section": "7.2 Practical",
    "text": "7.2 Practical\n\n7.2.0.1 Accuracy Assessment Result & Hyperparameter Tuning\nA reference dataset of Milan was introduced, defining a series of prediction bands, and the dataset was divided into a training and a test set, with 80% used for training and 20% for testing. A Random Forest classifier was trained for predicting the target category based on the input band data. The accuracy of the model was evaluated using the test set and a confusion matrix was generated to detail the performance of the model (results below).\n\n\n\n\n\nAccuracy Assessment Result\n\n\n\n\nThe overall accuracy of 83.33% indicates the proportion of samples correctly classified by the model. The model has high confidence for each predicted category with a Kappa value of 0.773 implying good agreement."
  },
  {
    "objectID": "Week_8.html#application",
    "href": "Week_8.html#application",
    "title": "7  Week_8",
    "section": "7.3 Application",
    "text": "7.3 Application\nObject-based image analysis (OBIA) has been widely used in remote sensing applications.OBIA methods have been applied to improve the classification accuracy of high-resolution (HR) remote sensing images by taking into account the spatial relationships between segmented objects and incorporating a priori knowledge. For example, one study proposed a novel classification scheme for HR remote sensing images that uses a knowledge graph (KG) to preserve spatial relationships and improve classification accuracy(Gun and Chen 2023). Another study evaluated a rule-based OBIA approach for landslide detection that combines probabilistic deep learning models with image segmentation and rule-based classification to improve accuracy(Ghorbanzadeh, Gholamnia, and Ghamisi 2023). OBIA has also been used for quantitative remote sensing, such as the design and analysis of the Miniature Multispectral Earth Observation Imager for Nanosatellites (PMEO)(Kivastik et al. 2022). In addition, OBIA has been integrated with a classifier integration strategy to improve land cover classification in complex urban areas using Very High Resolution (VHR) satellite data(Han et al. 2020).\nF1 scores are used in various applications of remote sensing. The F1 score is used to evaluate the performance of different feature selection methods in object-oriented remote sensing image classification experiments.The Fisher Score-mRMR (Fm) method combines the Fisher Score and the Minimum Redundancy Maximum Relevance (mRMR) feature selection method to improve the efficiency and accuracy of remote sensing image classification(Lv et al. 2022). In Through-the-Wall Radar Imaging (TWRI), the F1 score is used to evaluate the performance of the Compression Sensing (CS) algorithm. It is used to evaluate the algorithm’s ability to reconstruct images with correctly detected targets considering different levels of signal-to-noise ratio (SNR) and compression rate(John and Brad 2018). In the context of urban data classification, F1 scores are used to assess the effectiveness of feature reduction techniques in improving the accuracy of urban structure classification(Zemmoudj, Kemmouche, and Chibani 2014).\nSpatial cross-validation is an important technique in remote sensing classification. It helps to account for the presence of spatial autocorrelation in remotely sensed data and ensures unbiased estimation of prediction errors(Xudong et al. 2022). Several studies have highlighted the importance of spatial cross-validation in accurately assessing classification performance. For example, Karasiak et al. demonstrated that spatial leave-one-out cross-validation provides unbiased estimates of prediction error and is consistent with the true quality of the resulting maps(Karasiak et al. 2022). Similarly, Stock and Subramaniam proposed a method called iSLOOCV, which iterates and integrates a series of error estimates over a range of interval distances to account for spatial autocorrelation in ocean remote sensing data(Andy and Ajit 2022). It was found that stratified statistics-based sampling methods that take into account spatial dependence produce higher classification accuracy compared to other sample selection methods(Routh et al. 2018). These studies emphasise the importance of spatial cross-validation in accurately assessing classification performance and improving the reliability of remote sensing classification results."
  },
  {
    "objectID": "Week_8.html#reflection",
    "href": "Week_8.html#reflection",
    "title": "7  Week_8",
    "section": "7.4 Reflection",
    "text": "7.4 Reflection\nThe remote sensing learning journey is always exciting, not only because of the complexity of the technology itself and its applications in solving real-world problems, but also because of the opportunities for growth and awareness it gives us as students. Over the past week, I have delved into advanced remote sensing techniques such as Object-Based Image Analysis (OBIA), subpixel analysis, and hyperpixel generation algorithms, and practiced accuracy assessment methods, which has given me a deeper understanding of the potential and challenges of remote sensing.\nLearning the OBIA technique was a special experience that emphasised the importance of objects in image analysis rather than just individual pixels. This approach changed the way I interpret remote sensing data and made me realise that viewing images as a collection of interconnected objects rather than just pixel dots can greatly improve the accuracy and efficiency of analysis. Through specific case studies, such as how to incorporate deep learning models for landslide detection, I gained a more intuitive understanding of innovative applications in this field.\nIn particular, accuracy assessment through the use of random forest classifiers was extremely enlightening for me. Working with the data myself, tuning the model, and interpreting the confusion matrix all gave me a deep appreciation for the challenge and satisfaction of translating theoretical knowledge into practical application. It’s not just about running algorithms, it’s more about understanding the stories behind the data and how we can use these tools to reveal those stories.\nThe section discussing spatial cross-validation made me realise the importance of considering spatial autocorrelation when working with remotely sensed data. This is essential to improve the ability to generalise models and ensure the reliability of assessment results. Through this concept, I learnt how to validate models more effectively to ensure that they perform well on unknown data, not just on data that I have already been exposed to.\nFrom this learning experience, I gained valuable knowledge and a deeper love for the remote sensing discipline. I began to understand data more fully, analyse problems in more detail, and explore new solutions with greater confidence. This process is not just about the mastery of techniques, but also about how these techniques can be applied in the real world to make a positive impact. I look forward to applying what I have learnt to future projects, not only to solve specific problems, but also to further explore the potential of remote sensing technology."
  },
  {
    "objectID": "Week_8.html#reference",
    "href": "Week_8.html#reference",
    "title": "7  Week_8",
    "section": "7.5 Reference",
    "text": "7.5 Reference\n\n\n\n\nAndy, Stock, and Subramaniam Ajit. 2022. “Iterative Spatial Leave-One-Out Cross-Validation and Gap-Filling Based Data Augmentation for Supervised Learning Applications in Marine Remote Sensing.” https://www.tandfonline.com/doi/full/10.1080/15481603.2022.2107113.\n\n\nCsillik, Ovidiu. 2017. “Fast Segmentation and Classification of Very High Resolution Remote Sensing Data Using SLIC Superpixels.” Remote Sensing 9 (March): 243. https://doi.org/10.3390/rs9030243.\n\n\nGhorbanzadeh, Omid, Khalil Gholamnia, and Pedram Ghamisi. 2023. “The Application of ResU-Net and OBIA for Landslide Detection from Multi-Temporal Sentinel-2 Images.” Big Earth Data 7 (4): 961–85. https://doi.org/10.1080/20964471.2022.2031544.\n\n\nGun, Zhao, and Jianyu Chen. 2023. “Novel Knowledge Graph- and Knowledge Reasoning-Based Classification Prototype for OBIA Using High Resolution Remote Sensing Imagery.” Remote Sensing 15 (2): 321. https://doi.org/10.3390/rs15020321.\n\n\nHan, Ruimei, Pei Liu, Guangyan Wang, Hanwei Zhang, and Xilong Wu. 2020. “Advantage of Combining OBIA and Classifier Ensemble Method for Very High-Resolution Satellite Imagery Classification.” Journal of Sensors 2020 (November): e8855509. https://doi.org/10.1155/2020/8855509.\n\n\nJohn, F. Silny, and A. Flanders Brad. 2018. “Imaging Spectrometer f-Number Optimization for Remote Sensing of Gases.” https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10768/2323616/Imaging-spectrometer-F-number-optimization-for-remote-sensing-of-gases/10.1117/12.2323616.full#_=_.\n\n\nKarasiak, N., J.-F. Dejoux, C. Monteil, and D. Sheeren. 2022. “Spatial Dependence Between Training and Test Sets: Another Pitfall of Classification Accuracy Assessment in Remote Sensing.” Machine Learning 111 (7): 2715–40. https://doi.org/10.1007/s10994-021-05972-1.\n\n\nKivastik, Joosep, Hans Hubert Sams, Silvar Muru, Hendrik Ehrpais, Tõnis Eenmäe, Joel Kuusk, Andris Slavinskis, and Mihkel Pajusalu. 2022. “Optical Design and Analysis of Theia: A Scientific-Grade Multispectral Imager for Nanosatellites.” IEEE Journal on Miniaturization for Air and Space Systems 3 (4): 242–48. https://doi.org/10.1109/JMASS.2022.3210661.\n\n\nLv, Chengzhe, Yuefeng Lu, Miao Lu, Xinyi Feng, Huadan Fan, Changqing Xu, and Lei Xu. 2022. “A Classification Feature Optimization Method for Remote Sensing Imagery Based on Fisher Score and mRMR.” Applied Sciences 12 (September): 8845. https://doi.org/10.3390/app12178845.\n\n\nMeyer, Hanna, Christoph Reudenbach, Stephan Wöllauer, and Thomas Nauss. 2019. Importance of Spatial Predictor Variable Selection in Machine Learning Applications – Moving from Data Reproduction to Spatial Prediction.\n\n\nPriyankara, Prabath, Manjula Ranagalage, Dr Dissanayake, Takehiro Morimoto, and Yuji Murayama. 2019. “Spatial Process of Surface Urban Heat Island in Rapidly Growing Seoul Metropolitan Area for Sustainable Urban Planning Using Landsat Data.” Journal of Climate 7 (September): 110. https://doi.org/10.3390/cli7090110.\n\n\nRouth, Devin, Lindsi Seegmiller, Charlie Bettigole, Catherine Kuhn, Chadwick Oliver, and Henry Glick. 2018. “Improving the Reliability of Mixture Tuned Matched Filtering Remote Sensing Classification Results Using Supervised Learning Algorithms and Cross-Validation.” Remote Sensing 10 (October): 1675. https://doi.org/10.3390/rs10111675.\n\n\nXudong, Zhao, Zhang Mengmeng, Ran Tao Ran, Li Wei, and Philips Wilfried. 2022. “Cross-Domain Classification of Multisource Remote Sensing Data Using Fractional Fusion and Spatial-Spectral Domain Adaptation.” https://ieeexplore.ieee.org/document/9829269.\n\n\nZemmoudj, Salah, Akila Kemmouche, and Youeef Chibani. 2014. “2014 6th International Conference of Soft Computing and Pattern Recognition (SoCPaR).” In, 371–75. https://doi.org/10.1109/SOCPAR.2014.7008035."
  },
  {
    "objectID": "Week_6.html#reflection-未完成",
    "href": "Week_6.html#reflection-未完成",
    "title": "5  Week_6",
    "section": "5.4 Reflection 未完成",
    "text": "5.4 Reflection 未完成"
  },
  {
    "objectID": "Week_1.html#knowledge-from-the-lecture",
    "href": "Week_1.html#knowledge-from-the-lecture",
    "title": "1  Week_1",
    "section": "1.1 Knowledge From the Lecture",
    "text": "1.1 Knowledge From the Lecture\n\nThis week’s learning journey into the realm of remote sensing, unveiling the intricate world of electromagnetic waves and their interactions with Earth’s surface. Remote sensing, as I’ve learned, involves acquiring information about objects or areas from a distance, typically from aircraft or satellites, and is a vital tool in understanding our planet.\n\n\n1.1.1 Foundational Concepts of Remote Sensing\n\nOne of the foundational concepts we explored was the difference between passive and active remote sensing. Passive remote sensing relies on natural energy, usually from the sun, whereas active remote sensing systems emit their own energy to illuminate objects. This distinction is crucial for understanding the varied applications of remote sensing technologies like LiDAR, radar, and satellite imagery in observing earth’s landscapes, urban areas, and atmospheric conditions.\n\n\n\n\n\nPassive & Active Remote Sensing, Source: Abeer Nazar Abdul-Hameed\n\n\n\n\n\n\n\n1.1.2 Electromagnetic Waves: Core of Remote Sensing\n\nMuch of our learning knowledge focused on electromagnetic waves, the heart of remote sensing. These waves, a form of energy that propagates through electric and magnetic fields, are the backbone of how remote sensors collect data. Remote sensing is based on this principle to detect the reflection of electromagnetic waves by surface objects and their emission of electromagnetic waves, so as to extract information about these objects and complete the identification of objects at a distance.\n\n\n\n\n\n\nThe Electromagnetic Spectrum, Source: Cssonawala\n\n\n\n\n\n\n1.1.3 Interactions with Earth’s Surface\n\nWe have looked at the complexities of how electromagnetic radiation (EMR) interacts with the Earth’s surface. EMR can be absorbed, transmitted or scattered by the surface and atmosphere. Scattering, in particular, explains phenomena such as the blue colour of the sky or the blackness of the lunar sky due to the absence of an atmosphere.\n\n\n\n\n\n\nInteraction of EMR with Earth’s Surface, Source: Geographicbook\n\n\n\n\n\n\n1.1.4 Synthetic Aperture Radar (SAR) and its Applications\n\nThe Synthetic Aperture Radar (SAR) was another fascinating topic. SAR’s ability to ‘see through clouds’ using longer wavelengths is revolutionary, offering a consistent observation capability irrespective of weather conditions. This technology’s potential in areas like topography, vegetation analysis, and urban planning is immense, showcasing how advanced remote sensing techniques can overcome environmental challenges\nThe study of SAR data also introduced us to the concept of polarization, a property of electromagnetic waves that describes their oscillation direction. This property is crucial for understanding how radar signals interact with different surface properties and is instrumental in enhancing the efficiency and bandwidth of communications.\n\n\n\n1.1.5 Technical Insights and Data Formats in Remote Sensing\n\nOn the technical side, we learned about the different data formats used in remote sensing, such as GeoTIFF, and the importance of resolutions - spatial, spectral, temporal and radiometric. These resolutions define the quality and type of data acquired, and influence how effectively we can interpret and use the data for various applications such as land cover mapping and environmental monitoring.\nIn the literature, these concepts are used to analyse environmental change, urban development and geological features. Understanding remote sensing data and their interpretation is essential for researchers and policy makers to make informed decisions."
  },
  {
    "objectID": "Week_3.html#knowledge-from-the-lecture",
    "href": "Week_3.html#knowledge-from-the-lecture",
    "title": "3  Week_3",
    "section": "3.1 Knowledge From the Lecture",
    "text": "3.1 Knowledge From the Lecture\n\nThis week’s study focuses on remote sensing image processing, with the main areas of interest being geometric corrections, atmospheric corrections, orthometric corrections, radiometric corrections and various image enhancement techniques.\n\n\n3.1.1 Correction\n\n3.1.1.1 Geometric Correction\n\nGeometric correction is the basis of image correction and addresses image distortion due to sensor and other factors. The goal of this correction is to make the image conform to the selected map projection system, adjusting the geometry of the entire image by modelling the relationship between control points (points with known geographic coordinates) on the image and the image coordinates.\n\n\n\n\n\n\nHow Geometric Correction Work, Source: Xiaopeng\n\n\n\n\n\n\n3.1.1.2 Atmospheric Correction\n\nAtmospheric corrections are essential to address distortions and inaccuracies in remotely sensed images caused by variations in the Earth’s atmosphere, sensor angles, and terrain. Several different correction methods were learned in class: Relative Atmospheric Correction, Pseudo-Invariant Correction, Absolute Atmospheric Correction, and Empirical Line Correction.\n\nThe different methods and characteristics of atmospheric correction have been summarised in the table below:\n\n\n\n\n\n\n\n\nMethod\nKey Steps\nFeature\n\n\n\n\nRelative\nSpectrally stable landmarks, linear relations, band operation\nConsistency between images\n\n\nAbsolute\nComplex models for atmospheric effects, surface reflectance\nPrecise, accurate surface information\n\n\nPseudo-Invariant\nHigh-quality reference, PIF, linear regression\nStable reference points, reduce atmospheric effects\n\n\nEmpirical Line\nGround reflectance, average DN values, linear regression\nUtilizes ground data for satellite correction\n\n\n\n\n\n3.1.1.3 Radiometric Correction\n\nThe role of radiometric correction in adjusting the pixel values of satellite imagery to accurately reflect the radiation at the Earth’s surface has been investigated. The main objective of radiometric correction is to convert the raw digital numbers (DN) acquired into physically meaningful units such as radiance or reflectance. This conversion is critical because DN values are arbitrary and can vary between sensors, acquisitions and platforms, making it difficult to consistently compare and analyse data. By converting DN values to radiance or reflectance, radiometric correction enables quantitative measurements and meaningful comparisons between different images and sensors.\n\n\n\n\n\n\nProcess of Radiometric Calibration\n\n\n\n\n\n\n3.1.1.4 Reflection of Collerallation\n\nIn the course of study, certain questions arise as to when atmospheric corrections are needed and whether or not the images we buy or otherwise acquire have been atmospherically corrected. In layman’s terms, if we need to use an image that truly reflects the Sun’s radiation for quantitative inversion or to obtain information about the Earth, accurately identify features, etc., then we need to make an atmospheric correction. When we buy an image, the description document says that it is radiometrically corrected. In fact, this radiometric correction refers to the coarse radiometric correction, which is just a systematic atmospheric correction, which has the same meaning as the systematic geometric correction.\nAt the moment there are many models and methods on the market, since there are how many methods, then there is the problem of method selection. Here is a summary for reference:\n\nif it is a fine quantitative study, then choose the atmospheric correction method based on the radiative transfer model.\nif you are doing dynamic monitoring, then you can choose relative atmospheric correction or simpler methods.\nif the parameters are missing, there is no choice but to choose the simpler method.\n\n\n\n\n\n3.1.2 Data Join Sets/Enhancement\n\n3.1.2.1 Data Join Sets\n\nAn area may need more than one satellite image to be spliced, using mosaic in the R package for the two datasets to be spliced. When selecting images, try to select two images with similar time and date, due to the different date of the image, the image display is different, the reason may be (cloud cover, sunshine), need to use the image increase technique to operate.\n\n\n\n\n3.1.3 Enhancement\n\n3.1.3.1 Ratio\n\nThe ratio is the difference between two spectral bands with a specific spectral response, using CampTown’s data for NDVI (The Normalised Difference Vegetation Index) The Normalised Difference Vegetation Index is based on the fact that healthy and green vegetation reflects more in the near infrared but absorbs in the red wavelengths. Therefore, the red wavelength band is used for the operation in the formula below.\n\\[\nNDVI = \\frac{NIR - Red}{NIR + Red}\n\\]\nThe following figure shows the image after manipulation for NDVI (a. After NDVI Formula, b. Extraction only if NDVI is equal to or greater than 0.2)\n\n\n\n\n\n\nProcess of Radiometric Calibration\n\n\n\n\nThis case above can be applied to other index calculations such as NDWI (Normalised Difference Water Index) and NDDI (Normalised Difference Drought Index). These are calculated using the reflectance of different objects in different light waves.\n\n\n3.1.3.2 Texture\n\nThe extraction method of texture features is relatively simple, it is to use an active window to slide continuously on the image, calculate the variance, mean, maximum, minimum and the difference between the two and the information entropy in the window, etc., respectively, to form the corresponding texture image, when the spectral characteristics of the target are relatively close to each other, the texture features can play a positive role in distinguishing the target. When the spectral characteristics of the target are close, the texture features can play a positive role in distinguishing the target. After selecting the appropriate dynamic range of the data and extracting the texture features, the texture features of the image can be highlighted, which is conducive to the extraction of constructive information. Below is an example of a texture treatment for the Cape Town area.\n\n\n\n\n\n\nAfter of Texture Process"
  },
  {
    "objectID": "Week_6.html#knowledge-from-the-lecture",
    "href": "Week_6.html#knowledge-from-the-lecture",
    "title": "5  Week_6",
    "section": "5.1 Knowledge From the Lecture",
    "text": "5.1 Knowledge From the Lecture\n\nGoogle Earth Engine is a cloud-based platform provided by Google for online visual computation and analysis of large amounts of global-scale geoscience data (especially satellite data). It is characterised by allowing large-scale geospatial analyses, running very fast, having code that runs on the client side, and storing the data on the server.\n\n5.1.1 Raster and vector data in GEE\n\nRaster data, called “Image”, has bands.\nVector data, called “Feature”, has geometry and a dictionary of properties.。\n\n\n\n5.1.2 Image scaling in GEE\n\nRefers to the resolution of the image, i.e. the actual ground distance represented by each pixel.\nGEE automatically selects the appropriate zoom level based on the analysis requirements.\n\n\n\n5.1.3 Projection in GEE\nGEE supports a variety of projections, including Mercator projection, Albers projection, and isometric cylindrical projection. The user can select the appropriate projection for analysis.\n\n\n5.1.4 How to use GEE\n\n\n\n\n\nHow to use GEE, Source: Google Earth Engine\n\n\n\n\n\n\n5.1.5 Operations that can be performed in GEE\n\nGeometric operations: e.g. spatial operations, joins (Joins), region statistics (e.g. average temperature of a neighbourhood), filtering of images or specific values.\nMachine learning, both supervised and unsupervised, deep learning using TensorFlow, exploring relationships between variables.\nApplications/outputs: online graphs, scalable geospatial applications using GEE data\n\n\n\n5.1.6 Reduce the Image in GEE\nReducing the image by region and reducing the image by neighbour are both functions that are used to regionalise or neighbourhoodise an image. The main difference between them is:\n\nReducing the image by region operates on the image according to a specified region. Each region can be any shape and can overlap. The function performs the specified computation on all pixels within each region and returns a new image containing the statistics of the region.\nReducing the image by neighbour is to operate on the image according to the specified neighbourhood. The neighbourhood of each pixel is the pixels within a certain range around it. This function performs the specified computation on each pixel and all pixels within its neighbourhood, and returns an image containing the pixel’s new value.\n\n\n\n5.1.7 Linear Regression & Join in GEE (暂时写不出来)\n\n\n5.2 Practical\n\n5.2.1 Compare with RGB & NDVI\n\n\n\n\n\nCompare with RGB & NDVI Slide the slider to see the changes in RGB and NDVI.\n\n\n5.2.2 PCA Analysis\nSelecting a region, it first normalizes the image bands, calculates the covariance matrix, extracts the eigenvalues and eigenvectors, and then projects the image data into the principal component space. Finally, it displays the first principal component of the PCA result on the map.\n\n\n\n\n\nPCA Analysis\n\n\n\n\n\n\n\n5.3 Applications\n\n5.3.1 General Applications\nGEE has been the focus of attention in remote sensing big data processing.GEE is a cloud-based platform for parallel processing of geospatial data globally using Google’s cloud.GEE is a free cloud-based platform hosting more than 40 years of petabyte-sized remote sensing data, such as Landsat, MODIS, National Oceanic and Atmospheric Administration’s Advanced Ultra-High Resolution Radiometer (NOAA AVHRR), Sentinel 1, 2, 3, and 5-P; and Advanced Land Observing Satellite (ALOS) data(Aghamiri et al., n.d.). GEE also includes climate-weather and geophysical datasets. Other off-the-shelf products such as the Enhanced Vegetation Index (EVI) and the Normalised Vegetation Index (NDVI) are also available. In addition to having access to a large repository of raw remote sensing imagery, users can access pre-processed imagery, cloud removal imagery and mosaics in the GEE data catalogue(Ritika, n.d.).\n\n\n\n\n\nA summary of the algorithms and capabilities available in code editor-Google Earth Engine, Author: (Tamiminia et al. 2020)\n\n\n\n\nHere are some applications of GEE in practice.\n\n\n\n\n\nGEE in Different Applications, Author: (Tamiminia et al. 2020)\n\n\n\n\n\n\n5.3.2 GEE and data types\nMuch of the academic use of GEE has been optical, thanks in large part to the 40-year free archive of Landsat imagery, and optical remote sensing data is still the most commonly used data source.The GEE data catalogue provides optical satellite imagery from 1972 to the present day, allowing researchers to conduct Earth monitoring studies(Pham-Duc et al. 2023). In addition, the wide range of applications by GEE users shows that optical images are easier to process and interpret for non-remote sensing experts. This is one of the reasons why GEE has a global reach in a wide range of scientific fields. At the same time, the combination of SAR and optical satellite data can help researchers to solve problems such as cloud cover. Especially in the tropics, the efficacy of optical images can be greatly affected due to continuous cloud cover and situations such as forest fires. Therefore, combining optical and SAR data can improve the accuracy of classification and provide more information to monitor surface changes.(Lea et al. 2023).\n\n\n5.3.3 GEE and sensor type\nLandsat is considered an important source of remote sensing data in the GEE, as it provides a continuous image of the Earth’s surface.The Landsat 9 satellite will be launched in 2020 with the aim of continuing the Landsat programme’s key role in monitoring the Earth’s resources. Long-term land cover change studies can be carried out on a regional and global scale(Pham-Duc et al. 2023).\nSentinel-1 is another popular source of satellite imagery and has achieved very accurate classification results.Sentinel-1 consists of two satellites, Sentinel-1A and Sentinel 1-B, which were launched in 2014 and 2016, respectively, with a spatial resolution of 10 m and a revisit time of 6 days(Arias Cuenca 2023). It is equipped with a dual-polarised C-band SAR sensor that provides data in all-weather, day and night conditions.The Sentinel-2 mission consists of a constellation of two satellites, Sentinel-2A (launched in 2015) and Sentinel-2B (launched in 2017), which provide spatially resolved 10 m, 20 m and 60 m optical images, and temporal resolution of about 5 days(Xu, Heremans, and Somers 2022; Lechner et al. 2022).\nThe Moderate Resolution Imaging Spectroradiometer (MODIS) was launched on the Terra satellite in 1999 and on the Aqua satellite in 2002. Researchers can access MODIS data in GEE in 36 spectral bands and at three varying spatial resolutions (250 m, 500 m, and 1 km) at 1-day revisit times. Even though MODIS has a low spatial resolution, its high temporal resolution allows researchers to monitor short- and long-term global environmental change (dynamics)(Wu and Xiong 2020).\n\n\n5.3.4 Remote sensing data analysis\n\n5.3.4.1 Machine learning techniques\nMachine learning is a subset of artificial intelligence that deals with the design of algorithms to train models to make decisions or predictions. Machine learning methods can be divided into two broad categories: parametric and non-parametric. Parametric machine learning algorithms use a fixed number of parameters or assumptions. Machine learning methods have been effectively used for remote sensing data processing. Classification, clustering, regression and dimensionality reduction are the four main categories of analysis for machine learning algorithms(Shaveta 2023). Regression is a supervised machine learning method designed to estimate or predict output variables based on a set of covariates. Another positive aspect of linear regression is its fast computational speed, which is an important factor in geographic big data analysis(“Google Earth Engine and Machine Learning for Earth Monitoring,” n.d.). Also the MLR model deals with the non-linear relationship between the dependent and independent variables in the prediction process.\n\n\n5.3.4.2 Other GEE image processing functions\nGEE provides a wide range of image processing tools suitable for the analysis of remotely sensed data. These tools cover time series analysis, feature extraction, colour compositing of images and image preprocessing, mainly for satellite images, rather than machine learning-based methods(“Google Earth Engine and Machine Learning for Earth Monitoring,” n.d.). With the rapid changes in the Earth’s surface, time-series analysis of satellite imagery has become critical, helping to track trends, monitor changes and develop predictive models.GEE has been used for such analyses in a number of studies because of its ability to handle high resolution or large amounts of data, and is particularly good at monitoring changes in the land surface. Feature extraction techniques, by analysing spectral and geometric attributes of images, help to identify regional relationships in images, which is critical for resource conservation and information retention(Pham-Duc et al. 2023). In addition, GEE’s visual interpretation tools are widely used in land monitoring studies to extract key information from colour composite images. In addition, GEE’s image pre-processing capabilities support image mosaicing, cloud processing and error detection, and although cloud coverage is a major challenge when working with optical data, GEE provides effective tools and algorithms to support these tasks(Lea et al. 2023).\nOn the algorithmic side, there are challenges in implementing new algorithms, especially deep learning models. Although GEE has recently established a connection to TensorFlow, enabling users to interact with models stored on the Google AI platform, it does not directly support deep learning classifiers. Most research has used pixel-based classification methods and there is a need for better support for complex unsupervised classification algorithms as well as improvements in object-based image analysis, heavy vector manipulation, and the availability of high-resolution satellite imagery(Nia et al., n.d.).\n\n\n\n\n5.4 Reflection\nReflecting on the comprehensive insights gained from the past week’s deep dive into Google Earth Engine (GEE), my intellectual journey through the realm of remote sensing has been both illuminating and transformative. The exploration extended beyond the fundamental technicalities of GEE’s raster and vector data handling; it ventured into the practical applications and profound implications of this powerful tool in understanding and interacting with our planet.\nThe nuanced understanding of image reduction techniques—distinguishing between region-based and neighbor-based reductions—was particularly revelatory. It highlighted GEE’s nuanced capacity to distill vast amounts of geospatial data into actionable insights. This aspect of GEE not only enhances its analytical precision but also expands its utility across various research and operational contexts, enabling a more targeted and efficient analysis of environmental phenomena.\nThe enlightening exploration of GEE’s vast capabilities has significantly broadened my perspective, revealing the interdisciplinary potential of remote sensing technology. It’s fascinating to see how the integration of various data types—from climate variables to topographical features—can offer a more comprehensive understanding of environmental dynamics. This integrative approach not only enhances the accuracy of environmental assessments but also fosters a more holistic understanding of our planet’s intricate systems.\nThe reflective journey through this week has been a blend of learning, discovery, and inspiration. It has fortified my appreciation for the critical role of remote sensing in environmental science and its potential to inform and inspire sustainable interventions. The insights gained are not merely academic; they resonate with the pressing need for innovative solutions to global environmental challenges. As I look forward to the continuation of this academic journey, I am motivated by the potential to contribute meaningfully to our collective understanding and stewardship of the Earth, leveraging the powerful capabilities of GEE to illuminate the path toward a sustainable future.\nThis experiential learning has not only enriched my technical proficiency but also deepened my commitment to leveraging technology in pursuit of environmental sustainability. The profound capabilities of GEE—to process, analyze, and visualize complex geospatial datasets—offer a compelling glimpse into the future of environmental monitoring and analysis. Embracing this technology’s potential can revolutionize our approach to confronting and mitigating the pressing environmental challenges of our time."
  },
  {
    "objectID": "Week_7.html#knowledge-from-the-lecture",
    "href": "Week_7.html#knowledge-from-the-lecture",
    "title": "6  Week_7",
    "section": "6.1 Knowledge From the Lecture",
    "text": "6.1 Knowledge From the Lecture\n\n\n6.1.1 Classification and regression trees (CART)\n\n6.1.1.1 Classification\nClassification trees are used to categorise data into two or more discrete categories Regression trees deal with situations where linear regression does not apply Improve the predictive power of the model by splitting the data into smaller chunks When creating a decision tree, the final leaf nodes may be a mixture of categories (impurity) and the Gini impurity is used to quantify this impurity. Select the attribute with the lowest impurity as the top of the tree to begin the decision process. Calculate the Gini impurity and use it to assess the quality of the data segmented when constructing the decision tree, with smaller values indicating purer data\n\n\n\n\n\nLand Cover Classification using Google Earth Engine and Random Forest Classifier—The Role of Image Composition, Source: (Phan, Kuch, and Lehnert 2020)\n\n\n\n\n\n\n6.1.1.2 Regression trees\nRegression trees predict continuous values, such as the amount of pollution, while classification trees predict discrete values, such as land cover type. When linear regression does not fit the data well, regression trees are recommended as an alternative. In a regression tree, the data is divided into multiple parts based on thresholds or nodes. The sum of squared residuals (SSR) of these parts is calculated and the threshold with the lowest SSR becomes the starting point or root of the tree. The process can be repeated to further segment the data, and a minimum number of observations can be set to prevent overfitting.\n\n\n\n\n\nTree classification procedure in Google Earth Engine, Source: (Laengner, Siteur, and Wal 2019)\n\n\n\n\n\n\n\n6.1.2 Overfitting\nIf a leaf node contains only one person or one pixel value, overfitting may occur. The best models have low bias and low variability and are able to make consistent predictions across different datasets (e.g., training and test sets). To prevent overgrowth of the decision tree, its methods include limiting the growth of the tree (e.g., a leaf contains at least 20 pixels), and weakest link pruning (pruning based on the tree score). The number of leaves per tree and the value of α (regularisation parameter) were adjusted to reduce overfitting. Starting from α = 0, the α values were gradually increased until the pruning could reduce the tree score, and then these α values were saved. The tree score is the sum of squared residuals (SSR) plus the tree penalty (α multiplied by the number of leaves T). Different α values produce different subtrees and tree scores. Use different values of α to train the data and calculate the SSR on the test data to select the tree with the smallest score. Repeat the above process with cross-validation (10 cross validations) so as to find the α value that on average has the lowest SSR on the test data. The tree corresponding to this α-value, trained using all the data, is then selected. For classification trees, the SSR will be replaced by an impurity metric (e.g., Gini impurity).\n\n\n6.1.3 Random Tress\nA random forest consists of a number of categorical decision trees that are constructed by self-sampling the data (bootstrap samples) and constructing decision trees from randomly selected variables. At the nodes, the algorithm again selects from a random subset of variables. This process is repeated over and over again, resulting in multiple trees, or a “forest”. As new data passes through these trees, each tree gives a prediction, and the one with the most votes is chosen as the final prediction. The “bagging” technique in Random Forest is self-sampling by replacing data. Each tree is trained using approximately 70% of the training data, and the remaining 30% is called out-of-bag (OOB) data. The out-of-bag data is used to test the forest to evaluate the performance of the model and finally the classification result with the most votes is selected. The percentage of classification errors for out-of-bag data is known as OOB error. No pruning is done in a random forest and the tree can grow as much as possible. The out-of-bag error is derived by calculating the average prediction error for all trees that do not use certain values (e.g., rows in the data). Validation data, unlike out-of-bag data, is never included in the construction of the decision tree.\n\n\n6.1.4 How to apply to the imagery\nTwo main approaches to image classification: supervised learning and unsupervised learning. Supervised learning learns from data and labels new data through machine learning pattern recognition, while unsupervised learning analyses undefined data through clustering and then labels these clusters.\nSupervised Learning:\n\nGeneric of supervised learning basically follows the process includes: category definition, preprocessing, training, pixel assignment and accuracy assessment.\n\nUnsupervised Learning:\n\nThe DBSCAN algorithm, which forms clusters by setting a radius (Epsilon) and a minimum number of points, and can be optimised by iteration and PCA.\nThe ISODATA algorithm, a variant of k-means, which adds the ability to merge clusters that are too close together or to split clusters that are too long, and controls the clustering process according to the number of pixels in the cluster, the number of iterations, etc. 3. the “Cluster busting” algorithm, which forms clusters by setting a radius (Epsilon) and a minimum number of points.\nThe “Cluster busting” method, which improves classification accuracy by masking and reclassifying clusters that are difficult to label or incorrectly labelled.\n\nMaximum likelihood\nMaximum likelihood & Support Vector Machine Maximum Likelihood Estimation (MLE) is a statistical method for estimating parameters in probabilistic models. The basic idea of the method is to select the parameter value that best explains the observed data from all possible parameter values. In remote sensing, for example, it uses probabilities to assign each pixel in an image to the most likely land cover type, and probability thresholds can be set to determine whether or not to classify it.\n\n\n\n\n\nMaximum likelihood classifier, Source: (Núñez et al. 2019)\n\n\n\n\nSupport Vector Machine\nSupport Vector Machine (SVM) is a supervised learning model used for classification and regression analysis. Suppose we have a training dataset in which each data point belongs to one of two classes.The goal of the SVM is to find a hyperplane such that the hyperplane separates the two classes of data points as much as possible.\n\n\n\n\n\nSVM example of linearly separable data, Source: (Sheykhmousa and Mahdianpari 2020)\n\n\n\n\n\n\n6.2 Practical\n\n6.2.1 Supervised Classification\nSelect a Study Area, select the training feature collections on the map, in the following figure selected forest, water,developed,herbaceous as the collect feature.Use ee.Classifier.smileCart) and train it. But the result is not very good, maybe the initial data set selection is not very good.\n\n\n\n\n\nSupervised trained Classification Result\n\n\n\n\n\n\n6.2.2 Unsupervised Classification\nSame result of Unsupervised trained Classification\n\n\n\n\n\nUnsupervised trained Classification Result\n\n\n\n\n\n\n\n6.3 Application\nThe roots of remote sensing machine learning can be traced back to the 1990s. It was initially introduced as an approach to remote sensing for automated knowledge infrastructure building. Since then it has evolved and found applications in a variety of fields, including remote sensing and geoscience(Challa, Sridhar, and Shyam Mohan 2022). Machine learning algorithms such as deep learning are popular in remote sensing due to their ability to analyse large amounts of data and achieve high accuracy(Jeon 2023). These algorithms have been used for tasks such as image classification, scene understanding and material recognition(Rewhel et al. 2023). The availability of datasets with domain-specific attributes further facilitates the application of machine learning techniques in remote sensing.\nThere are three main categories of machine learning algorithms. One is supervised machine learning, second is unsupervised machine learning and third is reinforcement learning. The difference between supervised and unsupervised is that using supervised algorithms, there is a dataset containing output columns whereas in using unsupervised algorithms, there is only a huge dataset and its duty is to cluster the algorithms based on the relationship of the dataset to a variety of different classes between the different records that have been identified[Ling (2023)](Raju et al. 2023).\nRandom Forest algorithms are becoming increasingly popular in the remote sensing community due to their classification accuracy. These are integrated classifiers, which basically means that they utilise multiple decision trees underneath.One of the main reasons for the popularity of RF classifiers is that they help to mitigate high-dimensional problems(Bahrami, Hassani, and Maghsoudi 2018).They provide a variable importance (VI) that can reduce the dimensionality of hyperspectral data. Variable importance is essentially a measure of how a change in a particular input affects the output[Solorio-Ramírez et al. (2023)](Rina et al. 2023).\nSVMs are supervised learning models that can be used for regression and classification problems. They are mainly used for classification problems. They work by plotting points (features) in an n-dimensional space (features) and then dividing these points with a hyperplane. SVMs are used for almost all types of classification problems in remote sensing from forest classification to multispectral remote sensing image segmentation(Feizi and Nazemi 2022). Like other algorithms, their success depends on the nature of the problem, and one must test each algorithm separately and then make a decision based on each algorithm’s performance(Hazra et al. 2021).\nOverfitting a model usually requires building an overly complex model to account for characteristics and outliers in the study data. This means that if you evaluate the model with the same type of data (the type of data it has been trained on), you will get a very high prediction, classification, and classification accuracy(Schmidt, n.d.). However, if you just modify some inputs, (which the model hasn’t seen before), then the prediction, classification accuracy will drop. You can fix the overfitting by using a larger dataset and splitting the dataset appropriately. Also, it is useful to reduce the complexity of the model definition so that all extreme boundary cases are not classified(Rezaei and Sabokrou, n.d.).\n\n\n6.4 Reflection\n这节课主要学习了一些机器学习的技术在遥感中的应用，讲述了使用机器学习来解决什么样的问题。在上面所陈述的方法里面，哪个才是最适用的呢？这个问题的答案取决于一个人想要解决的问题。在某些情况下，当您有多个维度但记录有限时，SVM可能会更好地工作。如果你有很多的记录，但很少的维度(特性),神经网络(NN)可能产生更好的预测/分类精度。人们经常需要在你的数据集上测试多种算法，然后选择最有效的算法。通常，需要为不同的算法调整各种参数(i)。对射频、隐藏层数、神经网络神经元的数量以及对SVMs的”决策函数形状”等进行了研究。很多时候，将多个算法组合在一起可以获得更好的准确性，这就是所谓的合奏。还可以将SVM和神经网络、SVM和RF(可能性无穷)组合起来，以提高预测精度。再次，须测试多个合奏以选择最好的合奏。\n同样重要的是要注意,预测精度可能会改变根据特定功能试图使用分类、预测的目的而改变。例如，Shang和Chisholm(2014)讨论了如何将澳大利亚本土森林物种分类，他们决定使用最先进的遥感算法。在树叶、树冠和社区层面对树木进行分类。他们测试了各种算法(SVM、AdaBoost和Random Forest)，并发现每种算法在不同级别上都优于其他算法。在叶级，随机森林获得了最佳分类精度(94.7%)，支持向量机在冠层(84.5%)和社区水平(75.5%)的表现优于其他算法。\n另一个影响算法选择的因素是数据是否线性可分。例如，线性分类算法期望数据可以被线性空间中的直线分割。假设数据是线性可分的，可能适用于大多数情况，但在某些场景下是正确的,并会降低预测/分类精度。因此，我们需要确保使用的算法能够处理可用的数据。\n不可能只看一种算法，从理论上决定它是否会为你的数据集产生最好的结果，因为很多机器学习算法都是黑盒算法。这意味着很难看出算法是如何达到特定的结果的。因此，首先根据问题的类型来缩小算法选择的范围，然后在数据集的一部分应用缩小算法，看看哪一种性能最好。\n机器学习有着光明的未来，因为越来越多的人正在学习机器学习的基本知识，并将其应用于日常工作和研究中。新的算法每隔一天就会出现，分类的准确率也随之提高。这些问题在遥感(测绘地皮)中似乎很困难，有时甚至是不可能的，但每天都被新出现的算法解决。在不久的将来，世界上大多数的分析工作将由机器学习算法完成。"
  },
  {
    "objectID": "Week_8.html#knowledge-from-the-lecture",
    "href": "Week_8.html#knowledge-from-the-lecture",
    "title": "7  Week_8",
    "section": "7.1 Knowledge From the Lecture",
    "text": "7.1 Knowledge From the Lecture\n\n7.1.1 Object based image analysis (OBIA)\n\nThis is an analytical method that considers how ground objects are represented on raster cells. The Simple Linear Iterative Clustering algorithm is the most common method for generating hyperpixels. It segments the image into regions with similar colours and spatial locations called hyperpixels. Hyperpixel segmentation can be used for tasks such as image noise reduction, edge detection, texture analysis etc.The basic idea of SLIC algorithm is to iteratively update the clustering labels for each pixel point. In each iterative step, the algorithm calculates the distance of each pixel point from its neighbouring pixel points and assigns it to the closest cluster centre.\nMaybe this concept is a bit abstract for non-beginners, so I’ll explain it in more detail. Imagine you have an image that consists of many pixels. You want to divide these pixels into groups so that the pixels in each group have similar colours and spatial locations.\nThe SLIC algorithm is like a “game” of grouping pixels. The rules of the game are as follows:\n\nFirst, need to randomly select some points in the image as “cluster centres”. Then, you need to calculate the distance of each pixel from all the cluster centres.\nEach pixel will be assigned to the cluster centre closest to it.\nNext, the coordinates of each cluster centre need to be updated so that they are located at the average position of all pixels in the cluster.\nRepeat the above steps until all pixel points are assigned to a cluster centre.\nTour At the end of the game, a set of pixel points with similar colours and spatial locations, i.e., hyperpixels, will be obtained.\n\n\n\n\n\n\nComparison of Simple Linear Iterative Clustering (SLIC) and SLICO superpixel adherence to natural image boundaries derived using initial clustering of 10 × 10 pixels., Source: (Csillik 2017)\n\n\n\n\n\n7.1.2 Sub pixel analysis\nSubpixel analysis is a technique that analyses an image between its pixels. While traditional image processing methods focus only on the grey value of each pixel, subpixel analysis can take advantage of the subtle differences in grey scale between pixels to obtain more precise information. It has a wide range of applications, including: image enhancement, edge detection, texture analysis, target recognition.\n\n\n\n\n\nSuperpixel Generation Algorithm\n\n\n\n\n\n\n7.1.3 Assessment of the accuracy of classification of remotely sensed data\nDescribes how accuracy assessment is performed after producing remote sensing data as part of the machine learning workflow. Three of the key metrics are Producer’s Accuracy, User’s Accuracy, and Overall Accuracy, and how these metrics can be calculated using the Confusion Matrix.\n\n\n\n\n\n\n\nTerm\nDescription\n\n\n\n\nTrue Positive (TP)\nThe model correctly predicts the positive class.\n\n\nTrue Negative (TN)\nThe model correctly predicts the negative class.\n\n\nFalse Positive (FP)\nThe model incorrectly predicts positive, but the actual class is negative.\n\n\nFalse Negative (FN)\nThe model incorrectly predicts negative, but the actual class is positive.\n\n\n\nCalculation of the above indicators is in the following table\n\n\n\n\n\n\n\n\nAccuracy Metric\nFormula\nShort Definition\n\n\n\n\nProducer’s Accuracy\nTP / (TP + FN)\nCorrect classification proportion compared to ground truth.\n\n\nUser’s Accuracy\nTP / (TP + FP)\nCorrect classification proportion out of all classified.\n\n\nOverall Accuracy\n(TP + TN) / (TP + FP + FN + TN)\nProportion of all correctly classified pixels.\n\n\n\nOn this basis the Kappa coefficient can be used to carry out an effective assessment of the performance of the classification model to ensure the reliability and accuracy of the classification results. The value of the Kappa coefficient ranges from -1 (complete inconsistency) to 1 (complete agreement). A value of 0 indicates that the consistency is the same as random chance, while a value close to 1 indicates very high consistency. It is calculated as (actual consistency - random consistency)/(1 - random consistency).\n\n\n\n\n\nExample of Accuracies and kappa coefficient of land use:land cover (LULC) classifications in the SMA, Source: (Priyankara et al. 2019)\n\n\n\n\n\n\n7.1.4 F1 Score\nWhen the classes in your dataset are unbalanced, i.e. one class has far more samples than another, the kappa coefficient will not be used, but the F1 Score is particularly useful. In this case, using precision alone may not be sufficient to reflect the model’s performance, as the model may only perform well in predicting the dominant class while ignoring a few.The F1 Score provides a more comprehensive performance metric by balancing precision (the proportion of predicted-positive classes that are actually positive) and recall (the proportion of actual-positive classes that are predicted-positive). If precision and recall are equally valued in your task, i.e. you want to reduce the number of false positives and false negatives, then F1 Score is an appropriate choice. It ensures that you don’t sacrifice one of the metrics by improving the other. Although the F1 Score can be extended to multi-class classification problems, it is mainly suitable for binary classification problems, especially when the importance of positive and negative samples is essentially equal. The ROC curve can be invoked when studying binary classification problems, which provides a way to compare multiple classifiers under different class distributions or different cost/weight conditions. ROC curves and AUC values provide a consistent evaluation criterion for model comparisons, even when the data sets vary over time or across data sets.\n\n\n7.1.5 Spatial cross validation\nIn the traditional machine Learning cross-validation approach, the dataset is randomly divided into a training set and a test set. Models are trained on the training set and evaluated on the test set. However, this random division approach ignores a key property —— spatial autocorrelation in remotely sensed data, i.e., neighbouring regions often have similar attributes to each other.\nAs an example now there is a large remote sensing image that contains a variety of terrain such as forests, lakes and urban areas. The goal of the task is to create a computer model that can look at any part of this image and tell you exactly whether it is a forest, lake or city. To train this model, you need to select some samples from the image (i.e., a small part of the image) and tell the model which terrain each of these samples belongs to. The model then learns these samples and tries to understand the appearance of the different terrains.\nBut here’s the catch: if you randomly select samples, then samples that are very close together may appear in both the training data (the data the model uses to learn) and the test data (the data used to check the model’s accuracy). This is like knowing part of the test questions before you take the test, which may make the model seem to perform well, but in reality it may not have actually learnt how to differentiate between different terrains, but only remembered those particular samples.\nSpatial cross-validation is designed to solve this problem. Instead of randomly selecting samples, we divide the entire image into several large regions. We can make sure that certain regions are only used to train the model, while others are used to test the model. This way, we can be sure that the model is evaluated with data it has never seen before, which helps us to judge the actual performance of the model more accurately.\nFor example, suppose you have a large map containing cities, forests, and lakes. You divide the map into two parts, east and west. You train your model with the data from the eastern half, which means that the model will see and learn what the cities, forests, and lakes in this area look like. Then you test the model on the western half of the map to see if it can accurately recognise different terrain in areas it has never “seen” before. In this way, you can better assess how well the model actually performs when dealing with new and unknown areas.\n\n\n\n\n\nImportance of spatial predictor variable selection in machine learning applications – Moving from data reproduction to spatial prediction, Source: (Meyer et al. 2019)\n\n\n\n\n\n\n7.2 Practical\n\n7.2.0.1 Accuracy Assessment Result & Hyperparameter Tuning\nA reference dataset of Milan was introduced, defining a series of prediction bands, and the dataset was divided into a training and a test set, with 80% used for training and 20% for testing. A Random Forest classifier was trained for predicting the target category based on the input band data. The accuracy of the model was evaluated using the test set and a confusion matrix was generated to detail the performance of the model (results below).\n\n\n\n\n\nAccuracy Assessment Result\n\n\n\n\nThe overall accuracy of 83.33% indicates the proportion of samples correctly classified by the model. The model has high confidence for each predicted category with a Kappa value of 0.773 implying good agreement.\n\n\n\n7.3 Application\nObject-based image analysis (OBIA) has been widely used in remote sensing applications.OBIA methods have been applied to improve the classification accuracy of high-resolution (HR) remote sensing images by taking into account the spatial relationships between segmented objects and incorporating a priori knowledge. For example, one study proposed a novel classification scheme for HR remote sensing images that uses a knowledge graph (KG) to preserve spatial relationships and improve classification accuracy(Gun and Chen 2023). Another study evaluated a rule-based OBIA approach for landslide detection that combines probabilistic deep learning models with image segmentation and rule-based classification to improve accuracy(Ghorbanzadeh, Gholamnia, and Ghamisi 2023). OBIA has also been used for quantitative remote sensing, such as the design and analysis of the Miniature Multispectral Earth Observation Imager for Nanosatellites (PMEO)(Kivastik et al. 2022). In addition, OBIA has been integrated with a classifier integration strategy to improve land cover classification in complex urban areas using Very High Resolution (VHR) satellite data(Han et al. 2020).\nF1 scores are used in various applications of remote sensing. The F1 score is used to evaluate the performance of different feature selection methods in object-oriented remote sensing image classification experiments.The Fisher Score-mRMR (Fm) method combines the Fisher Score and the Minimum Redundancy Maximum Relevance (mRMR) feature selection method to improve the efficiency and accuracy of remote sensing image classification(Lv et al. 2022). In Through-the-Wall Radar Imaging (TWRI), the F1 score is used to evaluate the performance of the Compression Sensing (CS) algorithm. It is used to evaluate the algorithm’s ability to reconstruct images with correctly detected targets considering different levels of signal-to-noise ratio (SNR) and compression rate(John and Brad 2018). In the context of urban data classification, F1 scores are used to assess the effectiveness of feature reduction techniques in improving the accuracy of urban structure classification(Zemmoudj, Kemmouche, and Chibani 2014).\nSpatial cross-validation is an important technique in remote sensing classification. It helps to account for the presence of spatial autocorrelation in remotely sensed data and ensures unbiased estimation of prediction errors(Xudong et al. 2022). Several studies have highlighted the importance of spatial cross-validation in accurately assessing classification performance. For example, Karasiak et al. demonstrated that spatial leave-one-out cross-validation provides unbiased estimates of prediction error and is consistent with the true quality of the resulting maps(Karasiak et al. 2022). Similarly, Stock and Subramaniam proposed a method called iSLOOCV, which iterates and integrates a series of error estimates over a range of interval distances to account for spatial autocorrelation in ocean remote sensing data(Andy and Ajit 2022). It was found that stratified statistics-based sampling methods that take into account spatial dependence produce higher classification accuracy compared to other sample selection methods(Routh et al. 2018). These studies emphasise the importance of spatial cross-validation in accurately assessing classification performance and improving the reliability of remote sensing classification results.\n\n\n7.4 Reflection\nThe remote sensing learning journey is always exciting, not only because of the complexity of the technology itself and its applications in solving real-world problems, but also because of the opportunities for growth and awareness it gives us as students. Over the past week, I have delved into advanced remote sensing techniques such as Object-Based Image Analysis (OBIA), subpixel analysis, and hyperpixel generation algorithms, and practiced accuracy assessment methods, which has given me a deeper understanding of the potential and challenges of remote sensing.\nLearning the OBIA technique was a special experience that emphasised the importance of objects in image analysis rather than just individual pixels. This approach changed the way I interpret remote sensing data and made me realise that viewing images as a collection of interconnected objects rather than just pixel dots can greatly improve the accuracy and efficiency of analysis. Through specific case studies, such as how to incorporate deep learning models for landslide detection, I gained a more intuitive understanding of innovative applications in this field.\nIn particular, accuracy assessment through the use of random forest classifiers was extremely enlightening for me. Working with the data myself, tuning the model, and interpreting the confusion matrix all gave me a deep appreciation for the challenge and satisfaction of translating theoretical knowledge into practical application. It’s not just about running algorithms, it’s more about understanding the stories behind the data and how we can use these tools to reveal those stories.\nThe section discussing spatial cross-validation made me realise the importance of considering spatial autocorrelation when working with remotely sensed data. This is essential to improve the ability to generalise models and ensure the reliability of assessment results. Through this concept, I learnt how to validate models more effectively to ensure that they perform well on unknown data, not just on data that I have already been exposed to.\nFrom this learning experience, I gained valuable knowledge and a deeper love for the remote sensing discipline. I began to understand data more fully, analyse problems in more detail, and explore new solutions with greater confidence. This process is not just about the mastery of techniques, but also about how these techniques can be applied in the real world to make a positive impact. I look forward to applying what I have learnt to future projects, not only to solve specific problems, but also to further explore the potential of remote sensing technology."
  },
  {
    "objectID": "Week_3.html#reflection",
    "href": "Week_3.html#reflection",
    "title": "3  Week_3",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\n\nAs a student diving deep into remote sensing image correction technologies, it truly broadened my knowledge and also deepened my understanding of its complexities and challenges. I realized that although these technologies are crucial for enhancing the quality of image data, implementing them is not as straightforward as it might seem. For instance, radiometric correction sounds advanced, but if we lack precise ground objects for reference, the accuracy of the whole process becomes questionable. This got me thinking about whether we need to develop more adaptive correction methods that can adjust to varying conditions.\nThen there’s geometric correction, which indeed seems capable of solving issues with inconsistent image proportions, but if our initial data is not of good quality, or the processing is too complex, how do we balance precision with practical feasibility? It made me realize that sometimes we might need to find a balance between ideal accuracy and operational simplicity.\nWhen it comes to atmospheric correction, I was intrigued by the performance differences of various algorithms under specific conditions. This highlights the importance of selecting the right correction algorithm for reliable data, and also hints at the necessity for more detailed comparisons of algorithms in future research.\nRegarding image enhancement technologies, while they can make images appear clearer, I also began to worry about whether these techniques might inadvertently alter some crucial features in the images. This made me recognize the need to carefully maintain the authenticity of images while pursuing visual improvements.\nOverall, delving into remote sensing image correction technologies not only taught me about the importance of these techniques but also exposed the various challenges that need to be overcome in practical applications. It made me more aware that alongside technological innovation, a careful evaluation and improvement of existing methods are necessary to ensure we can enhance data quality while maintaining operational feasibility and data authenticity."
  },
  {
    "objectID": "Week_7.html#how-to-apply-to-the-imagery",
    "href": "Week_7.html#how-to-apply-to-the-imagery",
    "title": "6  Week_7",
    "section": "6.2 How to apply to the imagery",
    "text": "6.2 How to apply to the imagery\nTwo main approaches to image classification: supervised learning and unsupervised learning. Supervised learning learns from data and labels new data through machine learning pattern recognition, while unsupervised learning analyses undefined data through clustering and then labels these clusters.\nSupervised Learning:\n\nGeneric of supervised learning basically follows the process includes: category definition, preprocessing, training, pixel assignment and accuracy assessment.\n\nUnsupervised Learning:\n\nThe DBSCAN algorithm, which forms clusters by setting a radius (Epsilon) and a minimum number of points, and can be optimised by iteration and PCA.\nThe ISODATA algorithm, a variant of k-means, which adds the ability to merge clusters that are too close together or to split clusters that are too long, and controls the clustering process according to the number of pixels in the cluster, the number of iterations, etc. 3. the “Cluster busting” algorithm, which forms clusters by setting a radius (Epsilon) and a minimum number of points.\nThe “Cluster busting” method, which improves classification accuracy by masking and reclassifying clusters that are difficult to label or incorrectly labelled.\n\nMaximum likelihood\nMaximum likelihood & Support Vector Machine Maximum Likelihood Estimation (MLE) is a statistical method for estimating parameters in probabilistic models. The basic idea of the method is to select the parameter value that best explains the observed data from all possible parameter values. In remote sensing, for example, it uses probabilities to assign each pixel in an image to the most likely land cover type, and probability thresholds can be set to determine whether or not to classify it.\n\n\n\n\n\nMaximum likelihood classifier, Source: (Núñez et al. 2019)\n\n\n\n\nSupport Vector Machine\nSupport Vector Machine (SVM) is a supervised learning model used for classification and regression analysis. Suppose we have a training dataset in which each data point belongs to one of two classes.The goal of the SVM is to find a hyperplane such that the hyperplane separates the two classes of data points as much as possible.\n\n\n\n\n\nSVM example of linearly separable data, Source: (Sheykhmousa and Mahdianpari 2020)"
  },
  {
    "objectID": "Week_3.html#application",
    "href": "Week_3.html#application",
    "title": "3  Week_3",
    "section": "3.2 Application",
    "text": "3.2 Application\n\nImage correction is a critical step in remote sensing to ensure the accuracy and usability of data obtained from satellite or aerial imagery. Radiometric correction involves adjusting digital image data to correct for sensor noise, sensor response variations and atmospheric conditions. Various methods have been proposed for radiometric correction of remote sensing images. Duan(2014) proposed a radiation correction method that replaces artificial targets with standard ground objects, resulting in accurate and precise correction. Tarasenkov(2019) developed a program complex for atmospheric correction of satellite images that considers radiation polarization.\nGeometric correction corrects the image so that the proportions are uniform throughout the image. This correction is essential for accurate mapping and measurement tasks. Geometric correction is useful for removing spatial distortion, aligning images of the same sample, and stitching overlapping images together.(Yan et al. 2023). Various methods and devices have been developed for geometric correction, including those based on orthographic images and homonymous point matching. These techniques improve the accuracy and stability of the geometric information in the corrected image(Özciḣan et al. 2023).\nAtmospheric correction is necessary for various remote sensing applications to improve the accuracy and reliability of derived information. Different algorithms, such as 6S, FLAASH, DOS, LaSRC, and Sen2Cor, have been evaluated for their performance in atmospheric correction(Muchsin et al. 2023). Atmospheric corrections are essential for a variety of applications such as aircraft navigation, astronomical observations and accurate estimation of cloud heights(Shah, Raval, and Divakaran 2022). It ensures that satellite images provide reliable and accurate results by taking into account atmospheric disturbance(Jonah and Aketi, n.d.).\nImage enhancement technology aims to improve the quality and visual effect of remote sensing images. Various methods have been proposed to enhance these images, including histogram modification, transform domain methods, and hybrid approaches. Histogram modification methods focus on modifying the histogram of the input image to achieve a more uniform distribution, resulting in better contrast improvement. Transform domain methods apply specific transforms to enhance the image in the transform domain, leading to better edge enhancement and color preservation(Wang et al. 2023). Hybrid methods, which combine histogram modification and transform domain methods, have shown higher potential in enhancing remote sensing images(Deng et al. 2023). These enhancement techniques are crucial for image interpretation, improving image segmentation accuracy, and facilitating downstream tasks."
  }
]