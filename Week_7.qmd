---
title: "Week_7"
---

## **Knowledge From the Lecture**

::: {style="text-align: justify;"}
### Classification and regression trees (CART)

#### Classification

Classification trees are used to categorise data into two or more discrete categories Regression trees deal with situations where linear regression does not apply Improve the predictive power of the model by splitting the data into smaller chunks When creating a decision tree, the final leaf nodes may be a mixture of categories (impurity) and the Gini impurity is used to quantify this impurity. Select the attribute with the lowest impurity as the top of the tree to begin the decision process. Calculate the Gini impurity and use it to assess the quality of the data segmented when constructing the decision tree, with smaller values indicating purer data

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Land Cover Classification using Google Earth Engine and Random Forest Classifier—The Role of Image Composition, Source: <a href="https://www.mdpi.com/2072-4292/12/15/2411">[@phan2020]</a>'}
knitr::include_graphics('Figure/Week_7/Land Cover Classification in GEE.png')
```

#### Regression trees

Regression trees predict continuous values, such as the amount of pollution, while classification trees predict discrete values, such as land cover type. When linear regression does not fit the data well, regression trees are recommended as an alternative. In a regression tree, the data is divided into multiple parts based on thresholds or nodes. The sum of squared residuals (SSR) of these parts is calculated and the threshold with the lowest SSR becomes the starting point or root of the tree. The process can be repeated to further segment the data, and a minimum number of observations can be set to prevent overfitting.

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Tree classification procedure in Google Earth Engine, Source: <a href="https://www.researchgate.net/publication/334404370_Trends_in_the_Seaward_Extent_of_Saltmarshes_across_Europe_from_Long-Term_Satellite_Data">[@laengner2019]</a>'}
knitr::include_graphics('Figure/Week_7/Unsupervised-decision-tree-classification-procedure-in-Google-Earth-Engine-performed-for.ppm')
```

### Overfitting

If a leaf node contains only one person or one pixel value, overfitting may occur. The best models have low bias and low variability and are able to make consistent predictions across different datasets (e.g., training and test sets). To prevent overgrowth of the decision tree, its methods include limiting the growth of the tree (e.g., a leaf contains at least 20 pixels), and weakest link pruning (pruning based on the tree score). The number of leaves per tree and the value of α (regularisation parameter) were adjusted to reduce overfitting. Starting from α = 0, the α values were gradually increased until the pruning could reduce the tree score, and then these α values were saved. The tree score is the sum of squared residuals (SSR) plus the tree penalty (α multiplied by the number of leaves T). Different α values produce different subtrees and tree scores. Use different values of α to train the data and calculate the SSR on the test data to select the tree with the smallest score. Repeat the above process with cross-validation (10 cross validations) so as to find the α value that on average has the lowest SSR on the test data. The tree corresponding to this α-value, trained using all the data, is then selected. For classification trees, the SSR will be replaced by an impurity metric (e.g., Gini impurity).

### Random Tress

A random forest consists of a number of categorical decision trees that are constructed by self-sampling the data (bootstrap samples) and constructing decision trees from randomly selected variables. At the nodes, the algorithm again selects from a random subset of variables. This process is repeated over and over again, resulting in multiple trees, or a "forest". As new data passes through these trees, each tree gives a prediction, and the one with the most votes is chosen as the final prediction. The "bagging" technique in Random Forest is self-sampling by replacing data. Each tree is trained using approximately 70% of the training data, and the remaining 30% is called out-of-bag (OOB) data. The out-of-bag data is used to test the forest to evaluate the performance of the model and finally the classification result with the most votes is selected. The percentage of classification errors for out-of-bag data is known as OOB error. No pruning is done in a random forest and the tree can grow as much as possible. The out-of-bag error is derived by calculating the average prediction error for all trees that do not use certain values (e.g., rows in the data). Validation data, unlike out-of-bag data, is never included in the construction of the decision tree.

### How to apply to the imagery

Two main approaches to image classification: supervised learning and unsupervised learning. Supervised learning learns from data and labels new data through machine learning pattern recognition, while unsupervised learning analyses undefined data through clustering and then labels these clusters.

Supervised Learning:

1.  Generic of supervised learning basically follows the process includes: category definition, preprocessing, training, pixel assignment and accuracy assessment.

Unsupervised Learning:

1.  The DBSCAN algorithm, which forms clusters by setting a radius (Epsilon) and a minimum number of points, and can be optimised by iteration and PCA.
2.  The ISODATA algorithm, a variant of k-means, which adds the ability to merge clusters that are too close together or to split clusters that are too long, and controls the clustering process according to the number of pixels in the cluster, the number of iterations, etc. 3. the "Cluster busting" algorithm, which forms clusters by setting a radius (Epsilon) and a minimum number of points.
3.  The "Cluster busting" method, which improves classification accuracy by masking and reclassifying clusters that are difficult to label or incorrectly labelled.

**Maximum likelihood**

Maximum likelihood & Support Vector Machine Maximum Likelihood Estimation (MLE) is a statistical method for estimating parameters in probabilistic models. The basic idea of the method is to select the parameter value that best explains the observed data from all possible parameter values. In remote sensing, for example, it uses probabilities to assign each pixel in an image to the most likely land cover type, and probability thresholds can be set to determine whether or not to classify it.

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap=' Maximum likelihood classifier, Source: <a href="https://www.researchgate.net/figure/Maximum-likelihood-classifier-Source-adapted-from-59_fig2_331160604">[@núñez2019]</a>'}
knitr::include_graphics('Figure/Week_7/Maximum likelihood classifier.pbm')
```

**Support Vector Machine**

Support Vector Machine (SVM) is a supervised learning model used for classification and regression analysis. Suppose we have a training dataset in which each data point belongs to one of two classes.The goal of the SVM is to find a hyperplane such that the hyperplane separates the two classes of data points as much as possible.

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap=' SVM example of linearly separable data, Source: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206124">[@sheykhmousa2020]</a>'}
knitr::include_graphics('Figure/Week_7/SVM example of linearly separable data.png')
```

## **Practical**

### Supervised Classification

Select a Study Area, select the training feature collections on the map, in the following figure selected forest, water,developed,herbaceous as the collect feature.Use ee.Classifier.smileCart) and train it. But the result is not very good, maybe the initial data set selection is not very good.

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Supervised trained Classification Result'}
knitr::include_graphics('Figure/Week_7/Supervised trained Classification .png')
```

### Unsupervised Classification

Same result of Unsupervised trained Classification

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Unsupervised trained Classification Result'}
knitr::include_graphics('Figure/Week_7/Unsupervised trained Classification .png')
```

## **Application**

The roots of remote sensing machine learning can be traced back to the 1990s. It was initially introduced as an approach to remote sensing for automated knowledge infrastructure building. Since then it has evolved and found applications in a variety of fields, including remote sensing and geoscience[@challa2022]. Machine learning algorithms such as deep learning are popular in remote sensing due to their ability to analyse large amounts of data and achieve high accuracy[@jeon2023]. These algorithms have been used for tasks such as image classification, scene understanding and material recognition[@rewhel2023]. The availability of datasets with domain-specific attributes further facilitates the application of machine learning techniques in remote sensing.

There are three main categories of machine learning algorithms. One is supervised machine learning, second is unsupervised machine learning and third is reinforcement learning. The difference between supervised and unsupervised is that using supervised algorithms, there is a dataset containing output columns whereas in using unsupervised algorithms, there is only a huge dataset and its duty is to cluster the algorithms based on the relationship of the dataset to a variety of different classes between the different records that have been identified[@ling2023][@raju2023].

Random Forest algorithms are becoming increasingly popular in the remote sensing community due to their classification accuracy. These are integrated classifiers, which basically means that they utilise multiple decision trees underneath.One of the main reasons for the popularity of RF classifiers is that they help to mitigate high-dimensional problems[@bahrami2018].They provide a variable importance (VI) that can reduce the dimensionality of hyperspectral data. Variable importance is essentially a measure of how a change in a particular input affects the output[@solorio-ramírez2023][@rina2023].

SVMs are supervised learning models that can be used for regression and classification problems. They are mainly used for classification problems. They work by plotting points (features) in an n-dimensional space (features) and then dividing these points with a hyperplane. SVMs are used for almost all types of classification problems in remote sensing from forest classification to multispectral remote sensing image segmentation[@feizi2022]. Like other algorithms, their success depends on the nature of the problem, and one must test each algorithm separately and then make a decision based on each algorithm's performance[@hazra2021].

Overfitting a model usually requires building an overly complex model to account for characteristics and outliers in the study data. This means that if you evaluate the model with the same type of data (the type of data it has been trained on), you will get a very high prediction, classification, and classification accuracy[@schmidt]. However, if you just modify some inputs, (which the model hasn't seen before), then the prediction, classification accuracy will drop. You can fix the overfitting by using a larger dataset and splitting the dataset appropriately. Also, it is useful to reduce the complexity of the model definition so that all extreme boundary cases are not classified[@rezaei].

## **Reflection**

这节课主要学习了一些机器学习的技术在遥感中的应用，讲述了使用机器学习来解决什么样的问题。在上面所陈述的方法里面，哪个才是最适用的呢？这个问题的答案取决于一个人想要解决的问题。在某些情况下，当您有多个维度但记录有限时，SVM可能会更好地工作。如果你有很多的记录，但很少的维度(特性),神经网络(NN)可能产生更好的预测/分类精度。人们经常需要在你的数据集上测试多种算法，然后选择最有效的算法。通常，需要为不同的算法调整各种参数(i)。对射频、隐藏层数、神经网络神经元的数量以及对SVMs的"决策函数形状"等进行了研究。很多时候，将多个算法组合在一起可以获得更好的准确性，这就是所谓的合奏。还可以将SVM和神经网络、SVM和RF(可能性无穷)组合起来，以提高预测精度。再次，须测试多个合奏以选择最好的合奏。

同样重要的是要注意,预测精度可能会改变根据特定功能试图使用分类、预测的目的而改变。例如，Shang和Chisholm(2014)讨论了如何将澳大利亚本土森林物种分类，他们决定使用最先进的遥感算法。在树叶、树冠和社区层面对树木进行分类。他们测试了各种算法(SVM、AdaBoost和Random Forest)，并发现每种算法在不同级别上都优于其他算法。在叶级，随机森林获得了最佳分类精度(94.7%)，支持向量机在冠层(84.5%)和社区水平(75.5%)的表现优于其他算法。

另一个影响算法选择的因素是数据是否线性可分。例如，线性分类算法期望数据可以被线性空间中的直线分割。假设数据是线性可分的，可能适用于大多数情况，但在某些场景下是正确的,并会降低预测/分类精度。因此，我们需要确保使用的算法能够处理可用的数据。

不可能只看一种算法，从理论上决定它是否会为你的数据集产生最好的结果，因为很多机器学习算法都是黑盒算法。这意味着很难看出算法是如何达到特定的结果的。因此，首先根据问题的类型来缩小算法选择的范围，然后在数据集的一部分应用缩小算法，看看哪一种性能最好。

机器学习有着光明的未来，因为越来越多的人正在学习机器学习的基本知识，并将其应用于日常工作和研究中。新的算法每隔一天就会出现，分类的准确率也随之提高。这些问题在遥感(测绘地皮)中似乎很困难，有时甚至是不可能的，但每天都被新出现的算法解决。在不久的将来，世界上大多数的分析工作将由机器学习算法完成。
:::

## **Reference**
