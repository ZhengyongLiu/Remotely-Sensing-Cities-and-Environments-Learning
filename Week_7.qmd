---
title: "Week_7"
bibliography: references.bib
---

## Knowledge From the Lecture

**Classification and regression trees (CART)**

1.  Classification

分类树用于将数据分类到两个或更多的离散类别 回归树处理线性回归不适用的情况 通过将数据分割成小块来改进模型的预测能力 在创建决策树时，最终的叶子节点可能是类别的混合（不纯），并使用基尼不纯度来量化这种不纯度。选择最低不纯度的属性作为树的顶部来开始决策过程。 计算基尼不纯度，并用它来评估在构建决策树时分割数据的质量，其值越小表示数据越纯净。

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Land Cover Classification using Google Earth Engine and Random Forest Classifier—The Role of Image Composition, Source: <a href="https://www.mdpi.com/2072-4292/12/15/2411">[@phan2020]</a>'}
knitr::include_graphics('Figure/Week_7/Land Cover Classification in GEE.png')
```

2.  Regression trees

回归树预测连续值，例如污染量，而分类树预测离散值，例如土地覆盖类型。 当线性回归不能很好地拟合数据时，建议使用回归树作为替代方案。在回归树中，数据根据阈值或节点划分为多个部分。计算这些部分的残差平方和（SSR），并且具有最低SSR的阈值成为树的起点或根。可以重复该过程以进一步分割数据，并且可以设置最小观察次数以防止过度拟合。

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Tree classification procedure in Google Earth Engine, Source: <a href="https://www.researchgate.net/publication/334404370_Trends_in_the_Seaward_Extent_of_Saltmarshes_across_Europe_from_Long-Term_Satellite_Data">[@laengner2019]</a>'}
knitr::include_graphics('Figure/Week_7/Unsupervised-decision-tree-classification-procedure-in-Google-Earth-Engine-performed-for.ppm')
```

**Overfitting**

如果一个叶节点只包含一个人或一个像素值，就可能出现过拟合。最好的模型具有低偏差和低变异性，能够在不同数据集（如训练集和测试集）之间做出一致的预测。为了防止决策树过度生长的方法，其方法包括限制树的生长（例如，一个叶子至少包含20个像素），以及最弱连接剪枝（基于树得分的剪枝）。

每棵树的叶子数量和调整α值（正则化参数）来减少过拟合。从α=0开始，逐渐增加α值直到剪枝可以降低树得分，然后保存这些α值。树得分是残差平方和（SSR）加上树的惩罚（α乘以叶子数T）。不同的α值会产生不同的子树和树得分。使用不同的α值来训练数据，并在测试数据上计算SSR，以选择得分最小的树。用交叉验证（10次交叉验证）来重复上述过程，从而找到平均而言在测试数据上SSR最低的α值。然后选择这个α值对应的、使用全部数据训练的树。对于分类树，SSR将被不纯度度量（如基尼不纯度）所替代。

**Random Tress**

随机森林由许多分类决策树组成，通过对数据进行自助采样（bootstrap samples），并从随机选择的变量中构建决策树。在节点上，算法会再次从变量的随机子集中选择。这个过程会不断重复，最终得到多棵树，即一个"森林"。当有新数据通过这些树时，每棵树都会给出一个预测结果，最终选择票数最多的选项作为最终预测。

随机森林中的"bagging"技术，即通过替换数据进行自助采样。每棵树大约使用70%的训练数据进行训练，剩下30%的数据被称为袋外数据（OOB）。袋外数据被用来测试森林，以评估模型的性能，最后选择得票最多的分类结果。袋外数据分类错误的比例被称为OOB错误。

随机森林中不进行剪枝，树可以尽可能地生长。袋外错误是通过计算没有使用某些值（例如数据中的行）的所有树的平均预测错误来得出的。验证数据与袋外数据不同，它从未被包含在决策树的构建中。

**How to apply to the imagery**

图像分类的两种主要方法：监督学习和无监督学习。监督学习通过机器学习模式识别从数据中学习并对新数据打标签，而无监督学习则通过聚类分析未预先定义的数据，然后对这些聚类进行标签。

监督学习：

1.  监督学习的通用基本上都遵循流程包括：类别定义、预处理、训练、像素分配和准确性评估。

无监督学习:

1.  DBSCAN算法，它通过设定一个半径（Epsilon）和最小点数来形成聚类，并可通过迭代和PCA进行优化。
2.  ISODATA算法，k-means的一个变体，它增加了合并过近的聚类或分割过长的聚类的功能，并根据聚类中的像素数、迭代次数等条件来控制聚类过程.
3.  "Cluster busting"的方法，它通过掩盖和重新分类那些难以打标签或标签不正确的聚类来提高分类精度.

**Maximum likelihood**

Maximum likelihood & Support Vector Machine 最大似然估计（Maximum Likelihood Estimation，MLE）是一种统计方法，用于估计概率模型中的参数。该方法的基本思想是：从所有可能的参数值中，选择最能解释观察到的数据的参数值。例如在遥感中，它使用概率来将图像中的每个像素分配给最可能的土地覆盖类型，并可以设置概率阈值来决定是否进行分类。

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap=' Maximum likelihood classifier, Source: <a href="https://www.researchgate.net/figure/Maximum-likelihood-classifier-Source-adapted-from-59_fig2_331160604">[@núñez2019]</a>'}
knitr::include_graphics('Figure/Week_7/Maximum likelihood classifier.pbm')
```

**Support Vector Machine**

支持向量机（SVM）是一种监督学习模型，用于分类和回归分析。假设我们有一个训练数据集，其中每个数据点都属于两个类别中的一个。SVM 的目标是找到一个超平面，使得该超平面能够将两类数据点尽可能分开。

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap=' SVM example of linearly separable data, Source: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206124">[@sheykhmousa2020]</a>'}
knitr::include_graphics('Figure/Week_7/SVM example of linearly separable data.png')
```

## Practical

## Application

遥感机器学习的根源可以追溯到上世纪90年代。它最初被引入作为一种自动化知识基础建设的远程感知的方法。此后它不断发展并在各个领域找到了应用，包括遥感和地球科学[@challa2022]。深度学习等机器学习算法因其能够分析大量数据并实现高精度而在遥感领域广受欢迎[@jeon2023]。这些算法已用于图像分类、场景理解和材料识别等任务[@rewhel2023]。具有特定领域属性的数据集的可用性进一步促进了机器学习技术在遥感中的应用。

机器学习算法有三大类。一是监督机器学习，二是无监督机器学习，三是加强学习。监督和非监督的区别在于使用监督算法,有一个数据集包含的输出列而在使用无监督算法,一个只有一个巨大的数据集,它的职责是集群算法基于关系数据集到各种不同的类之间已经确定不同的记录[@ling2023][@raju2023]。

由于其分类的准确性，随机森林算法在遥感社区越来越受欢迎。这些是集成分类器，基本上意味着他们利用下面的多个决策树。RF分类器受欢迎的一个主要原因是它们有助于缓解高维问题[@bahrami2018]。它们提供了一个可变的重要性(VI)，可以减少高光谱数据的维数。变量的重要性本质上是衡量一个特定输入的变化对输出的影响[@solorio-ramírez2023;@rina2023]。

SVMs是监督学习模型，可用于回归和分类问题。它们主要用于分类问题。他们的工作方式是在一个n维空间(特征)中绘制的点(特征)，然后用一个超平面来划分这些点。从森林分类到多光谱遥感图像分割，在遥感中几乎所有类型的分类问题都使用SVMs[@feizi2022]。就像其他算法一样，他们的成功取决于问题的性质，一个人必须分别测试每个算法，然后根据每个算法的性能做出决定[@hazra2021]。

过度拟合模型通常需要建立一个过于复杂的模型来解释研究数据中的特性和异常值。这意味着,如果你使用相同类型的数据(它的数据类型已经训练)评估模型,你会得到一个非常高的预测、分类精度[@schmidt]。然而,如果你只是修改一些输入，(这模型没有见过)，那么，预测、分类精度就会下降。你可以通过使用更大的数据集来修复过度拟合，并适当地分割数据集。此外，减少模型定义的复杂性是有益的，这样就不会对所有极端的边界情况进行分类[@rezaei]。

## Reflection

这节课主要学习了一些机器学习的技术在遥感中的应用，讲述了使用机器学习来解决什么样的问题。在上面所陈述的方法里面，哪个才是最适用的呢？这个问题的答案取决于一个人想要解决的问题。在某些情况下，当您有多个维度但记录有限时，SVM可能会更好地工作。如果你有很多的记录，但很少的维度(特性),神经网络(NN)可能产生更好的预测/分类精度。人们经常需要在你的数据集上测试多种算法，然后选择最有效的算法。通常，需要为不同的算法调整各种参数(i)。对射频、隐藏层数、神经网络神经元的数量以及对SVMs的"决策函数形状"等进行了研究。很多时候，将多个算法组合在一起可以获得更好的准确性，这就是所谓的合奏。还可以将SVM和神经网络、SVM和RF(可能性无穷)组合起来，以提高预测精度。再次，须测试多个合奏以选择最好的合奏。

同样重要的是要注意,预测精度可能会改变根据特定功能试图使用分类、预测的目的而改变。例如，Shang和Chisholm(2014)讨论了如何将澳大利亚本土森林物种分类，他们决定使用最先进的遥感算法。在树叶、树冠和社区层面对树木进行分类。他们测试了各种算法(SVM、AdaBoost和Random Forest)，并发现每种算法在不同级别上都优于其他算法。在叶级，随机森林获得了最佳分类精度(94.7%)，支持向量机在冠层(84.5%)和社区水平(75.5%)的表现优于其他算法。

另一个影响算法选择的因素是数据是否线性可分。例如，线性分类算法期望数据可以被线性空间中的直线分割。假设数据是线性可分的，可能适用于大多数情况，但在某些场景下是正确的,并会降低预测/分类精度。因此，我们需要确保使用的算法能够处理可用的数据。

不可能只看一种算法，从理论上决定它是否会为你的数据集产生最好的结果，因为很多机器学习算法都是黑盒算法。这意味着很难看出算法是如何达到特定的结果的。因此，首先根据问题的类型来缩小算法选择的范围，然后在数据集的一部分应用缩小算法，看看哪一种性能最好。

机器学习有着光明的未来，因为越来越多的人正在学习机器学习的基本知识，并将其应用于日常工作和研究中。新的算法每隔一天就会出现，分类的准确率也随之提高。这些问题在遥感(测绘地皮)中似乎很困难，有时甚至是不可能的，但每天都被新出现的算法解决。在不久的将来，世界上大多数的分析工作将由机器学习算法完成。

## Reference
