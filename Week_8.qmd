---
title: "Week_8"
---

## **Knowledge From the Lecture**

### Object based image analysis (OBIA)

::: {style="text-align: justify;"}
This is an analytical method that considers how ground objects are represented on raster cells. The Simple Linear Iterative Clustering algorithm is the most common method for generating hyperpixels. It segments the image into regions with similar colours and spatial locations called hyperpixels. Hyperpixel segmentation can be used for tasks such as image noise reduction, edge detection, texture analysis etc.The basic idea of SLIC algorithm is to iteratively update the clustering labels for each pixel point. In each iterative step, the algorithm calculates the distance of each pixel point from its neighbouring pixel points and assigns it to the closest cluster centre.

Maybe this concept is a bit abstract for non-beginners, so I'll explain it in more detail. Imagine you have an image that consists of many pixels. You want to divide these pixels into groups so that the pixels in each group have similar colours and spatial locations.

The SLIC algorithm is like a "game" of grouping pixels. The rules of the game are as follows:

-   First, need to randomly select some points in the image as "cluster centres". Then, you need to calculate the distance of each pixel from all the cluster centres.

-   Each pixel will be assigned to the cluster centre closest to it.

-   Next, the coordinates of each cluster centre need to be updated so that they are located at the average position of all pixels in the cluster.

-   Repeat the above steps until all pixel points are assigned to a cluster centre.

-   Tour At the end of the game, a set of pixel points with similar colours and spatial locations, i.e., hyperpixels, will be obtained.

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Comparison of Simple Linear Iterative Clustering (SLIC) and SLICO superpixel adherence to natural image boundaries derived using initial clustering of 10 × 10 pixels., Source: <a href="https://www.researchgate.net/publication/314492084_Fast_Segmentation_and_Classification_of_Very_High_Resolution_Remote_Sensing_Data_Using_SLIC_Superpixels">[@csillik2017]</a>'}
knitr::include_graphics('Figure/Week_8/Comparison of Simple Linear Iterative Clustering (SLIC) and SLICO superpixel adherence to natural image boundaries derived using initial clustering of 10 × 10 pixels..png')
```

### Sub pixel analysis

Subpixel analysis is a technique that analyses an image between its pixels. While traditional image processing methods focus only on the grey value of each pixel, subpixel analysis can take advantage of the subtle differences in grey scale between pixels to obtain more precise information. It has a wide range of applications, including: image enhancement, edge detection, texture analysis, target recognition.

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Superpixel Generation Algorithm'}
knitr::include_graphics('Figure/Week_8/Superpixel Generation Algorithm.png')
```

### Assessment of the accuracy of classification of remotely sensed data

Describes how accuracy assessment is performed after producing remote sensing data as part of the machine learning workflow. Three of the key metrics are Producer's Accuracy, User's Accuracy, and Overall Accuracy, and how these metrics can be calculated using the Confusion Matrix.

| Term                | Description                                                                |
|--------------------|----------------------------------------------------|
| True Positive (TP)  | The model correctly predicts the positive class.                           |
| True Negative (TN)  | The model correctly predicts the negative class.                           |
| False Positive (FP) | The model incorrectly predicts positive, but the actual class is negative. |
| False Negative (FN) | The model incorrectly predicts negative, but the actual class is positive. |

Calculation of the above indicators is in the following table

| Accuracy Metric     | Formula                           | Short Definition                                            |
|-------------------|---------------------------|--------------------------|
| Producer's Accuracy | `TP / (TP + FN)`                  | Correct classification proportion compared to ground truth. |
| User's Accuracy     | `TP / (TP + FP)`                  | Correct classification proportion out of all classified.    |
| Overall Accuracy    | `(TP + TN) / (TP + FP + FN + TN)` | Proportion of all correctly classified pixels.              |

On this basis the Kappa coefficient can be used to carry out an effective assessment of the performance of the classification model to ensure the reliability and accuracy of the classification results. The value of the Kappa coefficient ranges from -1 (complete inconsistency) to 1 (complete agreement). A value of 0 indicates that the consistency is the same as random chance, while a value close to 1 indicates very high consistency. It is calculated as (actual consistency - random consistency)/(1 - random consistency).

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Example of Accuracies and kappa coefficient of land use:land cover (LULC) classifications in the SMA, Source: <a href="https://www.researchgate.net/figure/Accuracies-and-kappa-coefficient-of-land-use-land-cover-LULC-classifications-in-the_tbl2_335755723">[@priyankara2019]</a>'}
knitr::include_graphics('Figure/Week_8/Accuracies and kappa coefficient of land use:land cover (LULC) classifications in the SMA.png')
```

### F1 Score

When the classes in your dataset are unbalanced, i.e. one class has far more samples than another, the kappa coefficient will not be used, but the F1 Score is particularly useful. In this case, using precision alone may not be sufficient to reflect the model's performance, as the model may only perform well in predicting the dominant class while ignoring a few.The F1 Score provides a more comprehensive performance metric by balancing precision (the proportion of predicted-positive classes that are actually positive) and recall (the proportion of actual-positive classes that are predicted-positive). If precision and recall are equally valued in your task, i.e. you want to reduce the number of false positives and false negatives, then F1 Score is an appropriate choice. It ensures that you don't sacrifice one of the metrics by improving the other. Although the F1 Score can be extended to multi-class classification problems, it is mainly suitable for binary classification problems, especially when the importance of positive and negative samples is essentially equal. The ROC curve can be invoked when studying binary classification problems, which provides a way to compare multiple classifiers under different class distributions or different cost/weight conditions. ROC curves and AUC values provide a consistent evaluation criterion for model comparisons, even when the data sets vary over time or across data sets.

### Spatial cross validation

In the traditional machine Learning cross-validation approach, the dataset is randomly divided into a training set and a test set. Models are trained on the training set and evaluated on the test set. However, this random division approach ignores a key property ------ spatial autocorrelation in remotely sensed data, i.e., neighbouring regions often have similar attributes to each other.

As an example now there is a large remote sensing image that contains a variety of terrain such as forests, lakes and urban areas. The goal of the task is to create a computer model that can look at any part of this image and tell you exactly whether it is a forest, lake or city. To train this model, you need to select some samples from the image (i.e., a small part of the image) and tell the model which terrain each of these samples belongs to. The model then learns these samples and tries to understand the appearance of the different terrains.

But here's the catch: if you randomly select samples, then samples that are very close together may appear in both the training data (the data the model uses to learn) and the test data (the data used to check the model's accuracy). This is like knowing part of the test questions before you take the test, which may make the model seem to perform well, but in reality it may not have actually learnt how to differentiate between different terrains, but only remembered those particular samples.

Spatial cross-validation is designed to solve this problem. Instead of randomly selecting samples, we divide the entire image into several large regions. We can make sure that certain regions are only used to train the model, while others are used to test the model. This way, we can be sure that the model is evaluated with data it has never seen before, which helps us to judge the actual performance of the model more accurately.

For example, suppose you have a large map containing cities, forests, and lakes. You divide the map into two parts, east and west. You train your model with the data from the eastern half, which means that the model will see and learn what the cities, forests, and lakes in this area look like. Then you test the model on the western half of the map to see if it can accurately recognise different terrain in areas it has never "seen" before. In this way, you can better assess how well the model actually performs when dealing with new and unknown areas.

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Importance of spatial predictor variable selection in machine learning applications -- Moving from data reproduction to spatial prediction, Source: <a href="https://www.researchgate.net/figure/Concept-of-random-and-spatial-cross-validation-CV-A-total-dataset-here-9-different_fig3_335318909">[@meyer2019]</a>'}
knitr::include_graphics('Figure/Week_8/Concept of random and spatial cross-validation.png')
```

## **Practical**

#### Accuracy Assessment Result & Hyperparameter Tuning

A reference dataset of Milan was introduced, defining a series of prediction bands, and the dataset was divided into a training and a test set, with 80% used for training and 20% for testing. A Random Forest classifier was trained for predicting the target category based on the input band data. The accuracy of the model was evaluated using the test set and a confusion matrix was generated to detail the performance of the model (results below).

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Accuracy Assessment Result'}
knitr::include_graphics('Figure/Week_8/Accuracy Assessment Result.png')
```

The overall accuracy of 83.33% indicates the proportion of samples correctly classified by the model. The model has high confidence for each predicted category with a Kappa value of 0.773 implying good agreement.

## **Application**

Object-based image analysis (OBIA) has been widely used in remote sensing applications.OBIA methods have been applied to improve the classification accuracy of high-resolution (HR) remote sensing images by taking into account the spatial relationships between segmented objects and incorporating a priori knowledge. For example, one study proposed a novel classification scheme for HR remote sensing images that uses a knowledge graph (KG) to preserve spatial relationships and improve classification accuracy[@gun2023]. Another study evaluated a rule-based OBIA approach for landslide detection that combines probabilistic deep learning models with image segmentation and rule-based classification to improve accuracy[@ghorbanzadeh2023]. OBIA has also been used for quantitative remote sensing, such as the design and analysis of the Miniature Multispectral Earth Observation Imager for Nanosatellites (PMEO)[@kivastik2022]. In addition, OBIA has been integrated with a classifier integration strategy to improve land cover classification in complex urban areas using Very High Resolution (VHR) satellite data[@han2020].

F1 scores are used in various applications of remote sensing. The F1 score is used to evaluate the performance of different feature selection methods in object-oriented remote sensing image classification experiments.The Fisher Score-mRMR (Fm) method combines the Fisher Score and the Minimum Redundancy Maximum Relevance (mRMR) feature selection method to improve the efficiency and accuracy of remote sensing image classification[@lv2022]. In Through-the-Wall Radar Imaging (TWRI), the F1 score is used to evaluate the performance of the Compression Sensing (CS) algorithm. It is used to evaluate the algorithm's ability to reconstruct images with correctly detected targets considering different levels of signal-to-noise ratio (SNR) and compression rate[@john2018]. In the context of urban data classification, F1 scores are used to assess the effectiveness of feature reduction techniques in improving the accuracy of urban structure classification[@zemmoudj2014].

Spatial cross-validation is an important technique in remote sensing classification. It helps to account for the presence of spatial autocorrelation in remotely sensed data and ensures unbiased estimation of prediction errors[@xudong2022]. Several studies have highlighted the importance of spatial cross-validation in accurately assessing classification performance. For example, Karasiak et al. demonstrated that spatial leave-one-out cross-validation provides unbiased estimates of prediction error and is consistent with the true quality of the resulting maps[@karasiak2022]. Similarly, Stock and Subramaniam proposed a method called iSLOOCV, which iterates and integrates a series of error estimates over a range of interval distances to account for spatial autocorrelation in ocean remote sensing data[@andy2022]. It was found that stratified statistics-based sampling methods that take into account spatial dependence produce higher classification accuracy compared to other sample selection methods[@routh2018]. These studies emphasise the importance of spatial cross-validation in accurately assessing classification performance and improving the reliability of remote sensing classification results.

## **Reflection**

The remote sensing learning journey is always exciting, not only because of the complexity of the technology itself and its applications in solving real-world problems, but also because of the opportunities for growth and awareness it gives us as students. Over the past week, I have delved into advanced remote sensing techniques such as Object-Based Image Analysis (OBIA), subpixel analysis, and hyperpixel generation algorithms, and practiced accuracy assessment methods, which has given me a deeper understanding of the potential and challenges of remote sensing.

Learning the OBIA technique was a special experience that emphasised the importance of objects in image analysis rather than just individual pixels. This approach changed the way I interpret remote sensing data and made me realise that viewing images as a collection of interconnected objects rather than just pixel dots can greatly improve the accuracy and efficiency of analysis. Through specific case studies, such as how to incorporate deep learning models for landslide detection, I gained a more intuitive understanding of innovative applications in this field.

In particular, accuracy assessment through the use of random forest classifiers was extremely enlightening for me. Working with the data myself, tuning the model, and interpreting the confusion matrix all gave me a deep appreciation for the challenge and satisfaction of translating theoretical knowledge into practical application. It's not just about running algorithms, it's more about understanding the stories behind the data and how we can use these tools to reveal those stories.

The section discussing spatial cross-validation made me realise the importance of considering spatial autocorrelation when working with remotely sensed data. This is essential to improve the ability to generalise models and ensure the reliability of assessment results. Through this concept, I learnt how to validate models more effectively to ensure that they perform well on unknown data, not just on data that I have already been exposed to.

From this learning experience, I gained valuable knowledge and a deeper love for the remote sensing discipline. I began to understand data more fully, analyse problems in more detail, and explore new solutions with greater confidence. This process is not just about the mastery of techniques, but also about how these techniques can be applied in the real world to make a positive impact. I look forward to applying what I have learnt to future projects, not only to solve specific problems, but also to further explore the potential of remote sensing technology.
:::

## **Reference**
