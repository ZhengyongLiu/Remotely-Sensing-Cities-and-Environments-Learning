---
title: "Week_8 Classification II"
---

## **Knowledge From the Lecture**

### Object based image analysis (OBIA)

::: {style="text-align: justify;"}
This is an analytical method that considers how ground objects are represented on raster cells. The Simple Linear Iterative Clustering algorithm is the most common method for generating hyperpixels. It segments the image into regions with similar colours and spatial locations called hyperpixels. Hyperpixel segmentation can be used for tasks such as image noise reduction, edge detection, texture analysis etc.The basic idea of SLIC algorithm is to iteratively update the clustering labels for each pixel point. In each iterative step, the algorithm calculates the distance of each pixel point from its neighbouring pixel points and assigns it to the closest cluster centre.

The SLIC algorithm is like a "game" of grouping pixels. The rules of the game are as follows:

-   First, need to randomly select some points in the image as "cluster centres". Then, you need to calculate the distance of each pixel from all the cluster centres.

-   Each pixel will be assigned to the cluster centre closest to it.

-   Next, the coordinates of each cluster centre need to be updated so that they are located at the average position of all pixels in the cluster.

-   Repeat the above steps until all pixel points are assigned to a cluster centre.

-   Tour At the end of the game, a set of pixel points with similar colours and spatial locations, i.e., hyperpixels, will be obtained.

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Comparison of Simple Linear Iterative Clustering (SLIC) and SLICO superpixel adherence to natural image boundaries derived using initial clustering of 10 × 10 pixels., Source: <a href="https://www.researchgate.net/publication/314492084_Fast_Segmentation_and_Classification_of_Very_High_Resolution_Remote_Sensing_Data_Using_SLIC_Superpixels">[@csillik2017]</a>'}
knitr::include_graphics('Figure/Week_8/Comparison of Simple Linear Iterative Clustering (SLIC) and SLICO superpixel adherence to natural image boundaries derived using initial clustering of 10 × 10 pixels..png')
```

### Sub pixel analysis

Subpixel analysis is a technique that analyses an image between its pixels. While traditional image processing methods focus only on the grey value of each pixel, subpixel analysis can take advantage of the subtle differences in grey scale between pixels to obtain more precise information. It has a wide range of applications, including: image enhancement, edge detection, texture analysis, target recognition.

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Superpixel Generation Algorithm'}
knitr::include_graphics('Figure/Week_8/Superpixel Generation Algorithm.png')
```

### Assessment of the accuracy of classification of remotely sensed data

Describes how accuracy assessment is performed after producing remote sensing data as part of the machine learning workflow. Three of the key metrics are Producer's Accuracy, User's Accuracy, and Overall Accuracy, and how these metrics can be calculated using the Confusion Matrix.

| Term                | Description                                                                |
|--------------------|----------------------------------------------------|
| True Positive (TP)  | The model correctly predicts the positive class.                           |
| True Negative (TN)  | The model correctly predicts the negative class.                           |
| False Positive (FP) | The model incorrectly predicts positive, but the actual class is negative. |
| False Negative (FN) | The model incorrectly predicts negative, but the actual class is positive. |

On this basis the Kappa coefficient can be used to carry out an effective assessment of the performance of the classification model to ensure the reliability and accuracy of the classification results. The value of the Kappa coefficient ranges from -1 (complete inconsistency) to 1 (complete agreement). A value of 0 indicates that the consistency is the same as random chance, while a value close to 1 indicates very high consistency. It is calculated as (actual consistency - random consistency)/(1 - random consistency).

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Example of Accuracies and kappa coefficient of land use:land cover (LULC) classifications in the SMA, Source: <a href="https://www.researchgate.net/figure/Accuracies-and-kappa-coefficient-of-land-use-land-cover-LULC-classifications-in-the_tbl2_335755723">[@priyankara2019]</a>'}
knitr::include_graphics('Figure/Week_8/Accuracies and kappa coefficient of land use:land cover (LULC) classifications in the SMA.png')
```

### F1 Score

When the classes in your dataset are unbalanced, i.e. one class has far more samples than another, the kappa coefficient will not be used, but the F1 Score is particularly useful. In this case, using precision alone may not be sufficient to reflect the model's performance, as the model may only perform well in predicting the dominant class while ignoring a few.The F1 Score provides a more comprehensive performance metric by balancing precision (the proportion of predicted-positive classes that are actually positive) and recall (the proportion of actual-positive classes that are predicted-positive). If precision and recall are equally valued in your task, i.e. you want to reduce the number of false positives and false negatives, then F1 Score is an appropriate choice. It ensures that you don't sacrifice one of the metrics by improving the other. Although the F1 Score can be extended to multi-class classification problems, it is mainly suitable for binary classification problems, especially when the importance of positive and negative samples is essentially equal. The ROC curve can be invoked when studying binary classification problems, which provides a way to compare multiple classifiers under different class distributions or different cost/weight conditions. ROC curves and AUC values provide a consistent evaluation criterion for model comparisons, even when the data sets vary over time or across data sets.

### Spatial cross validation

In the traditional machine Learning cross-validation approach, the dataset is randomly divided into a training set and a test set. Models are trained on the training set and evaluated on the test set. However, this random division approach ignores a key property ------ spatial autocorrelation in remotely sensed data, i.e., neighbouring regions often have similar attributes to each other.

As an example now there is a large remote sensing image that contains a variety of terrain such as forests, lakes and urban areas. The goal of the task is to create a computer model that can look at any part of this image and tell you exactly whether it is a forest, lake or city. To train this model, you need to select some samples from the image (i.e., a small part of the image) and tell the model which terrain each of these samples belongs to. The model then learns these samples and tries to understand the appearance of the different terrains.

But here's the catch: if you randomly select samples, then samples that are very close together may appear in both the training data (the data the model uses to learn) and the test data (the data used to check the model's accuracy). This is like knowing part of the test questions before you take the test, which may make the model seem to perform well, but in reality it may not have actually learnt how to differentiate between different terrains, but only remembered those particular samples.

Spatial cross-validation is designed to solve this problem. Instead of randomly selecting samples, we divide the entire image into several large regions. We can make sure that certain regions are only used to train the model, while others are used to test the model. This way, we can be sure that the model is evaluated with data it has never seen before, which helps us to judge the actual performance of the model more accurately.

For example, suppose you have a large map containing cities, forests, and lakes. You divide the map into two parts, east and west. You train your model with the data from the eastern half, which means that the model will see and learn what the cities, forests, and lakes in this area look like. Then you test the model on the western half of the map to see if it can accurately recognise different terrain in areas it has never "seen" before. In this way, you can better assess how well the model actually performs when dealing with new and unknown areas.

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Importance of spatial predictor variable selection in machine learning applications -- Moving from data reproduction to spatial prediction, Source: <a href="https://www.researchgate.net/figure/Concept-of-random-and-spatial-cross-validation-CV-A-total-dataset-here-9-different_fig3_335318909">[@meyer2019]</a>'}
knitr::include_graphics('Figure/Week_8/Concept of random and spatial cross-validation.png')
```

## **Practical**

### Accuracy Assessment Result & Hyperparameter Tuning

A reference dataset of Milan was introduced, defining a series of prediction bands, and the dataset was divided into a training and a test set, with 80% used for training and 20% for testing. A Random Forest classifier was trained for predicting the target category based on the input band data. The accuracy of the model was evaluated using the test set and a confusion matrix was generated to detail the performance of the model (results below).

```{r  echo=FALSE, out.width='110%', fig.align='center', fig.cap='Accuracy Assessment Result'}
knitr::include_graphics('Figure/Week_8/Accuracy Assessment Result.png')
```

The overall accuracy of 83.33% indicates the proportion of samples correctly classified by the model. The model has high confidence for each predicted category with a Kappa value of 0.773 implying good agreement.

## **Application**
Object-based image analysis (OBIA) techniques have been widely used in remote sensing, especially in improving the accuracy of high-resolution (HR) remote sensing image classification. By considering the spatial relationship between segmented objects and integrating the a priori knowledge, the OBIA method not only improves the classification accuracy, but also enhances the recognition of complex surface features. For example, Gun and Chen [-@gun2023] proposed a novel classification scheme for HR remotely sensed images, which maintains spatial relationships and significantly improves classification accuracy by utilising a knowledge graph (KG). In addition, Ghorbanzadeh, Gholamnia, and Ghamisi [-@ghorbanzadeh2023] evaluated a rule-based OBIA landslide detection method that combines a probabilistic deep learning model with image segmentation and rule-based classification to effectively improve the detection accuracy.

The application of F1 score in remote sensing image analysis shows that it is an important tool for evaluating the performance of different feature selection methods in object-oriented remote sensing image classification experiments.The F1 score provides a balanced and reliable performance evaluation metric in remote sensing by combining precision and recall.Lv et al. [-@lv2022] provide a balanced and reliable performance evaluation metric in remote sensing by combining the Fisher Score and the minimum redundancy maximum relevance (mRMR) feature selection method, showing how the F1 score can be used to improve the efficiency and accuracy of remote sensing image classification. Meanwhile, in the Through-the-Wall Radar Imaging (TWRI) technique, the F1 score is used to evaluate the ability of compression-aware (CS) algorithms to reconstruct images at different signal-to-noise ratios (SNR) and compression rates, as studied by John and Brad[-@john2018].

The importance of spatial cross-validation as an important technique in remote sensing data classification is that it provides an unbiased estimate of the prediction error and ensures the reliability of remote sensing data analysis. This approach pays special attention to the spatial autocorrelation of the data and ensures the generalisation ability of the classification model through spatial leave-one-out cross-validation or similar techniques.Karasiak et al. [-@karasiak2022] showed how spatial leave-one-out cross-validation can provide unbiased estimation of the prediction error and ensures the fidelity of the resultant maps. Similarly, Routh et al. [-@routh2018] showed through the iSLOOCV method how spatial autocorrelation can be taken into account in marine remote sensing data to improve classification accuracy through the integration of iterative error estimation over an interval range.

## **Reflection**
After this week study, I've realized the profound impact of these advanced techniques on our ability to interpret and analyze environmental data. The intricacies of Object-based Image Analysis (OBIA) and subpixel analysis, beyond their technicalities, resonate with me as they unveil a deeper layer of interaction with the physical world, transforming mere pixels into a vivid narrative of the Earth's surface. Delving into accuracy assessment and spatial cross-validation has shifted my perspective, emphasizing that the true measure of a model's strength lies in its generalizability and its fidelity to unseen data. This realization challenges me to think critically about the models I build, pushing me to ensure they are not just statistically accurate but truly reflective of the complex patterns they aim to represent. Engaging with these concepts has not only broadened my technical knowledge but also deepened my appreciation for the meticulous balance between computational rigor and the interpretative richness that remote sensing offers, guiding me towards a more thoughtful, analytical approach in my future endeavors in this field.
:::

## **Reference**
